{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-bZSR4U8huU9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import tqdm\n",
    "import functools\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import dataclasses\n",
    "import transformers\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "from model import TransformerConfig, TransformerForCausalLM, TransformerForSequenceClassification\n",
    "\n",
    "from utils.logger import Logger\n",
    "from utils.metrics import best_subspan_exact_match\n",
    "from utils.etc import print_model_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java version: 21.0.6-internal\n"
     ]
    }
   ],
   "source": [
    "from jnius import autoclass\n",
    "\n",
    "System = autoclass('java.lang.System')\n",
    "print(\"Java version:\", System.getProperty('java.version'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/guest2/kangjun/defaultproject\n"
     ]
    }
   ],
   "source": [
    "PROJ_PATH = os.path.abspath(\".\")\n",
    "print(PROJ_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176,
     "referenced_widgets": [
      "1fb18ced426147489838b9f9538dcae1",
      "005aa25bb61f48ac8efb19f4512cebf3",
      "28d3585998804f6aa98d06db6e6d0e36",
      "7ad0b2f49a1248e0a7819d0f3083e12a",
      "2af23fa0a09941459d0995e913c0a9aa",
      "adb42c480d7f4773a1234c6061f4828f",
      "ab44e6727c654c75bc507ba55f208a25",
      "6100350a45274402a7c1dd7b9ce32336",
      "bfc8e0b5310749c4a0be9a31b3e39218",
      "dc9bbc5ef68f45fdb439cdd062d54778",
      "b6d61efd5f7f4970bb4176dd9a72b1fe"
     ]
    },
    "executionInfo": {
     "elapsed": 2360,
     "status": "ok",
     "timestamp": 1745441801038,
     "user": {
      "displayName": "Seung-Hoon Na",
      "userId": "02716637662108767334"
     },
     "user_tz": -540
    },
    "id": "rVRsEFk0huU9",
    "outputId": "b8223446-84e3-4de2-c7bb-4fa3db6f02bd"
   },
   "outputs": [],
   "source": [
    "MODEL_CONTEXT_LENGTH = 1024\n",
    "ROPE_THETA = 20000.0 # set 50k for 2048 context length, set 20k for 1024 context length\n",
    "## path needs to be modified for each environment.\n",
    "DRIVE_CACHE_PATH = \"/mountpoint/lkj/NLP\"\n",
    "LOCAL_CACHE_PATH = \"local_cache\"\n",
    "LOG_PATH = os.path.join(\"/mountpoint/lkj/NLP\", \"logs\")\n",
    "OUTPUT_PATH = os.path.join(\"/mountpoint/lkj/NLP\", \"output\")\n",
    "RESULTS_PATH = os.path.join(\"/mountpoint/lkj/NLP\", \"output\", \"results\")\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "os.makedirs(DRIVE_CACHE_PATH, exist_ok=True)\n",
    "os.makedirs(LOG_PATH, exist_ok=True)\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# only set matmul precision \"high\" for ampere and above GPUs(e.g. A100, V100, RTX 30xx, RTX 40xx) unless set \"highest\"\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "DO_PRETRAIN = False\n",
    "DO_FINETUNE_SM = False\n",
    "DO_FINETUNE_CF = False\n",
    "DO_FINETUNE_RAG = False\n",
    "DO_ZEROSHOT_RAG = False\n",
    "DO_PRE_FINETUNED_RAG = False # do LoRA finetune in pretrained model and finetune rag.\n",
    "DO_SUBMISSION = False\n",
    "DO_SUMMARY_LORA_FINETUNE = False # LoRA finetuning on summary and evaluate\n",
    "DO_RAG_LORA_FINETUNE = False # LoRA finetuning on trained rag and evaluate\n",
    "DO_ZEROSHOT_RAG_PROMPT = False # Change Prompt for Zeroshot\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1745428311125,
     "user": {
      "displayName": "Seung-Hoon Na",
      "userId": "02716637662108767334"
     },
     "user_tz": -540
    },
    "id": "xNIS8TU-huU9",
    "outputId": "230f225e-d99f-4f20-d915-9b79a96b5e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens: {'pad_token': '<|padding|>'}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
    "\n",
    "additional_special_tokens = {}\n",
    "if tokenizer.pad_token is None:\n",
    "    additional_special_tokens[\"pad_token\"] = \"<|padding|>\"\n",
    "if tokenizer.eos_token is None:\n",
    "    additional_special_tokens[\"eos_token\"] = \"<|endoftext|>\"\n",
    "if tokenizer.bos_token is None:\n",
    "    additional_special_tokens[\"bos_token\"] = \"<|beginoftext|>\"\n",
    "if additional_special_tokens:\n",
    "    print(f\"Adding special tokens: {additional_special_tokens}\")\n",
    "    tokenizer.add_special_tokens(additional_special_tokens)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.model_max_length = MODEL_CONTEXT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cjdjdI43huU-"
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class TrainingConfig(object):\n",
    "    # Device Setting\n",
    "    device: str = \"cuda\"\n",
    "    model_dtype:str = \"cast_bf16\"\n",
    "    # set \"cast_bf16\" if device is upper than ampere architecture for best performance unless use \"amp_fp16\" for mixed precision\n",
    "    # ampere architecture means nvidia a100, a40, a6000 or rtx 3000 series or higher\n",
    "\n",
    "    # Training setting\n",
    "    batch_size: int = 32\n",
    "    eval_batch_size: int = 32\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    num_train_epochs: int = 5\n",
    "    max_steps: int = None\n",
    "\n",
    "    # Optimizer setting\n",
    "    optimizer_type: str = \"adamw\"\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_steps: int = 1000\n",
    "    max_grad_norm:float = 3.0\n",
    "    lr_scheduler_type: str = \"linear\"\n",
    "\n",
    "    # Trainer setting\n",
    "    metric_for_best_model: str = \"loss\"\n",
    "    metric_greater_is_better: bool = False\n",
    "    # save_interval: int = 1000\n",
    "    eval_interval: int = 1000\n",
    "    logging_interval: int = 10\n",
    "    save_total_limit: int = 5\n",
    "\n",
    "    logging_path: str = os.path.join(LOG_PATH, \"train\")\n",
    "    output_path: str = os.path.join(LOG_PATH, \"output\")\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"model_dtype\": self.model_dtype,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"eval_batch_size\": self.eval_batch_size,\n",
    "            \"gradient_accumulation_steps\": self.gradient_accumulation_steps,\n",
    "            \"num_train_epochs\": self.num_train_epochs,\n",
    "            \"max_steps\": self.max_steps,\n",
    "            \"optimizer_type\": self.optimizer_type,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"max_grad_norm\": self.max_grad_norm,\n",
    "            \"lr_scheduler_type\": self.lr_scheduler_type,\n",
    "            \"metric_for_best_model\": self.metric_for_best_model,\n",
    "            \"metric_greater_is_better\": self.metric_greater_is_better,\n",
    "            \"eval_interval\": self.eval_interval,\n",
    "            \"logging_interval\": self.logging_interval,\n",
    "            \"save_total_limit\": self.save_total_limit,\n",
    "        }\n",
    "\n",
    "    def from_dict(self, config_dict):\n",
    "        for key, value in config_dict.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "            else:\n",
    "                raise KeyError(f\"Invalid key: {key} in config_dict\")\n",
    "        self.__post_init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgOMCEoshuU-"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_for_summary(model, dataloader, train_config, tokenizer, max_new_tokens=128):\n",
    "    if train_config.model_dtype == \"cast_bf16\":\n",
    "        model = model.to(torch.bfloat16)\n",
    "\n",
    "    model = model.to(train_config.device)\n",
    "    model.eval()\n",
    "    model.compile()\n",
    "\n",
    "    prediction = []\n",
    "    answers = []\n",
    "    for batch in tqdm.auto.tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "        batch = {k:v.to(train_config.device) for k,v in batch.items()}\n",
    "        input_ids = batch.pop(\"input_ids\")\n",
    "        labels = batch.pop(\"labels\")\n",
    "        attention_mask = batch.pop(\"attention_mask\")\n",
    "\n",
    "        generation_input_ids = input_ids.clone()\n",
    "        generation_attention_mask = attention_mask.clone()\n",
    "        generation_input_ids[labels.ne(-100)] = tokenizer.pad_token_id\n",
    "        generation_attention_mask[labels.ne(-100)] = 0\n",
    "\n",
    "        max_length = generation_attention_mask.sum(-1).max().item()\n",
    "        refit_generation_input_ids = torch.zeros((input_ids.shape[0], max_length), dtype=torch.long, device=input_ids.device)\n",
    "        refit_generation_attention_mask = torch.zeros((input_ids.shape[0], max_length), dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        for i in range(input_ids.shape[0]):\n",
    "            length = generation_attention_mask[i].sum().item()\n",
    "            if length == 0:\n",
    "                continue\n",
    "            refit_generation_input_ids[i, -length:] = generation_input_ids[i, generation_attention_mask[i].eq(1)]\n",
    "            refit_generation_attention_mask[i, -length:] = generation_attention_mask[i, generation_attention_mask[i].eq(1)]\n",
    "\n",
    "\n",
    "        gen_output = model.generate(\n",
    "            input_ids=refit_generation_input_ids,\n",
    "            attention_mask=refit_generation_attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            return_response_only=True\n",
    "        )\n",
    "\n",
    "        labels[labels.eq(-100)] = tokenizer.pad_token_id\n",
    "        pred = tokenizer.batch_decode(gen_output, skip_special_tokens=True)\n",
    "        ans = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        prediction.extend(pred)\n",
    "        answers.extend(ans)\n",
    "\n",
    "    rouge_score = rouge.compute(predictions=prediction, references=answers, use_stemmer=True)\n",
    "    metrics = {\n",
    "        \"rouge1\": rouge_score[\"rouge1\"].item(),\n",
    "        \"rouge2\": rouge_score[\"rouge2\"].item(),\n",
    "        \"rougeL\": rouge_score[\"rougeL\"].item(),\n",
    "        \"rougeLsum\": rouge_score[\"rougeLsum\"].item(),\n",
    "        \"prediction\": prediction,\n",
    "        \"answers\": answers,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_for_classification(model, dataloader, train_config):\n",
    "    if train_config.model_dtype == \"cast_bf16\":\n",
    "        model = model.to(torch.bfloat16)\n",
    "\n",
    "    model = model.to(train_config.device)\n",
    "    model.eval()\n",
    "    model.compile()\n",
    "\n",
    "    prediction = []\n",
    "    answers = []\n",
    "    for batch in tqdm.auto.tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "        batch = {k:v.to(train_config.device) for k,v in batch.items()}\n",
    "        input_ids = batch.pop(\"input_ids\")\n",
    "        labels = batch.pop(\"labels\")\n",
    "        attention_mask = batch.pop(\"attention_mask\")\n",
    "\n",
    "        logits,_ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pred = logits.argmax(-1)\n",
    "\n",
    "        prediction.extend(pred.tolist())\n",
    "        answers.extend(labels.tolist())\n",
    "\n",
    "    prediction = np.array(prediction)\n",
    "    answers = np.array(answers)\n",
    "\n",
    "    accuracy = (prediction == answers).sum() / len(answers)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"prediction\": prediction,\n",
    "        \"answers\": answers,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_for_rag(model, dataloader, train_config, tokenizer):\n",
    "    if train_config.model_dtype == \"cast_bf16\":\n",
    "        model.model = model.model.to(torch.bfloat16)\n",
    "\n",
    "    model.model = model.model.to(train_config.device)\n",
    "    model.model.eval()\n",
    "    model.model.compile()\n",
    "\n",
    "    uids = []\n",
    "    questions = []\n",
    "    predictions = []\n",
    "    answers = []\n",
    "    for batch in tqdm.auto.tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "        uid = batch['uid']\n",
    "        question = batch['question']\n",
    "        answer = batch['answers']\n",
    "\n",
    "        # retrieval_augmented_generate\n",
    "        extra_kw_args = {\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        } if isinstance(model.model, transformers.generation.GenerationMixin) else {}\n",
    "        outputs = model.retrieval_augmented_generate(\n",
    "            queries=question,\n",
    "            qids=uid,\n",
    "            max_new_tokens=10,\n",
    "            k=5,\n",
    "            **extra_kw_args\n",
    "        )\n",
    "\n",
    "        pred = model.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        uids.extend(uid)\n",
    "        questions.extend(question)\n",
    "        predictions.extend(pred)\n",
    "        answers.extend(answer)\n",
    "\n",
    "    # fill here\n",
    "\n",
    "    #######!\n",
    "\n",
    "    predictions = [pred.lower().strip().replace('<|endoftext|>', '').split(\"\\n\")[0] for pred in predictions]\n",
    "    answers = [[ans.lower().strip().replace('<|endoftext|>', '').split(\"\\n\")[0] for ans in group] for group in answers]\n",
    "    \n",
    "    #######!\n",
    "\n",
    "\n",
    "    accuracy = best_subspan_exact_match(predictions, answers)\n",
    "    rouge_score = rouge.compute(predictions=predictions, references=answers)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy['acc'],\n",
    "        \"rouge1\": rouge_score[\"rouge1\"].item(),\n",
    "        \"rouge2\": rouge_score[\"rouge2\"].item(),\n",
    "        \"rougeL\": rouge_score[\"rougeL\"].item(),\n",
    "        \"rougeLsum\": rouge_score[\"rougeLsum\"].item(),\n",
    "        \"prediction\": predictions,\n",
    "        \"answers\": answers,\n",
    "        \"uid\": uids,\n",
    "        \"question\": questions,\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jt-kO6hyhuU-"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loop_for_loss(model, dataloader, train_config, tokenizer):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in tqdm.auto.tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "        batch = {k:v.to(train_config.device) for k,v in batch.items()}\n",
    "\n",
    "        loss, _ = model(**batch)\n",
    "        losses.append(loss.item())\n",
    "    metrics = {\n",
    "        \"loss\": sum(losses)/len(losses),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loop_for_pretraining(model, dataloader, train_config, tokenizer):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    ppls = []\n",
    "\n",
    "    token_correct = 0\n",
    "    token_total = 0\n",
    "\n",
    "    first_batch = True # debugging\n",
    "    for batch in tqdm.auto.tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "        batch = {k:v.to(train_config.device) for k,v in batch.items()}\n",
    "        labels = batch.pop(\"labels\")\n",
    "        logits, _ = model(**batch)\n",
    "\n",
    "        shifted_logits = logits[..., :-1, :].contiguous()\n",
    "        shifted_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(shifted_logits.view(-1, shifted_logits.size(-1)), shifted_labels.view(-1),ignore_index=-100, reduction=\"none\")\n",
    "\n",
    "        loss = loss.view(shifted_labels.size())\n",
    "        mask = shifted_labels.ne(-100)\n",
    "\n",
    "        nll_loss = (loss * mask).sum() / mask.sum()\n",
    "        ppl = torch.exp(nll_loss)\n",
    "\n",
    "        losses.append(nll_loss.mean())\n",
    "        ppls.append(ppl.mean())\n",
    "\n",
    "        # ##\n",
    "        # preds = shifted_logits.argmax(dim=-1)\n",
    "        # correct = (preds == shifted_labels) & mask \n",
    "        # token_correct += correct.sum().item()\n",
    "        # token_total += mask.sum().item()\n",
    "        # ##\n",
    "\n",
    "        token_correct += (shifted_logits.argmax(-1) == shifted_labels).sum().item()\n",
    "        token_total += shifted_labels.ne(-100).sum().item()\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": torch.mean(torch.stack(losses)).item(),\n",
    "        \"ppl\": torch.mean(torch.stack(ppls)).item(),\n",
    "        \"token_acc\": token_correct / token_total,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train(model, train_dataset, eval_dataset, train_collate_fn, eval_collate_fn, train_config, eval_loop):\n",
    "    os.makedirs(train_config.logging_path, exist_ok=True)\n",
    "    os.makedirs(train_config.output_path, exist_ok=True)\n",
    "\n",
    "    train_model = model if isinstance(model, transformers.PreTrainedModel) else model.model\n",
    "    if train_config.model_dtype == \"cast_bf16\":\n",
    "        train_model = train_model.to(torch.bfloat16)\n",
    "\n",
    "    print_model_statistics(train_model)\n",
    "\n",
    "    train_model = train_model.to(train_config.device)\n",
    "    train_model.train()\n",
    "    train_model.compile()\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=train_config.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=train_collate_fn,\n",
    "    )\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=train_config.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=eval_collate_fn,\n",
    "    )\n",
    "\n",
    "    num_training_steps = len(train_loader) * train_config.num_train_epochs if train_config.max_steps is None else train_config.max_steps\n",
    "    num_warmup_steps = train_config.warmup_steps\n",
    "    num_train_epochs = train_config.num_train_epochs if train_config.max_steps is None else math.ceil(train_config.max_steps / len(train_loader))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        train_model.parameters(),\n",
    "        lr=train_config.learning_rate,\n",
    "        weight_decay=train_config.weight_decay,\n",
    "    )\n",
    "    scheduler = transformers.get_scheduler(\n",
    "        train_config.lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps // train_config.gradient_accumulation_steps,\n",
    "    )\n",
    "\n",
    "    global_steps = 0\n",
    "\n",
    "    logger = Logger(log_path=train_config.logging_path)\n",
    "    with open(os.path.join(train_config.output_path, \"train_config.json\"), \"w\") as f:\n",
    "        json.dump(train_config.to_dict(), f, indent=4)\n",
    "    logger.log({\"train_config\": train_config.to_dict()})\n",
    "\n",
    "    global_pbar = tqdm.auto.tqdm(total=num_training_steps, desc=\"Training\")\n",
    "    best_models = []\n",
    "    loss_window = []\n",
    "    for epoch in range(num_train_epochs):\n",
    "        train_model.train()\n",
    "        for batch in train_loader:\n",
    "            batch = {k:v.to(train_config.device) for k,v in batch.items()}\n",
    "\n",
    "            with torch.autocast(device_type=train_config.device, dtype=torch.float16, enabled=train_config.model_dtype == \"amp_fp16\"):\n",
    "                loss, _ = train_model(**batch)\n",
    "                l = loss.item()\n",
    "                loss_window.append(l)\n",
    "                if len(loss_window) > 100:\n",
    "                    loss_window.pop(0)\n",
    "\n",
    "            loss = loss / train_config.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if train_config.max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(train_model.parameters(), train_config.max_grad_norm)\n",
    "\n",
    "            if global_steps % train_config.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            global_steps += 1\n",
    "            global_pbar.update(1)\n",
    "            global_pbar.set_postfix({\"epoch\":epoch, \"loss\": f\"{l:.4f}::{sum(loss_window)/len(loss_window):.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"})\n",
    "\n",
    "            if global_steps % train_config.logging_interval == 0:\n",
    "                logger.log({\"steps\": global_steps, \"loss\": l,\"ave_loss\":sum(loss_window)/len(loss_window), \"lr\": scheduler.get_last_lr()[0]})\n",
    "\n",
    "            if global_steps % train_config.eval_interval == 0:\n",
    "                global_pbar.disable = True\n",
    "                global_pbar.refresh()\n",
    "                eval_metrics = eval_loop(model, eval_loader, train_config, tokenizer)\n",
    "                if \"prediction\" in eval_metrics: eval_metrics.pop(\"prediction\")\n",
    "                if \"answers\" in eval_metrics: eval_metrics.pop(\"answers\")\n",
    "                if \"uid\" in eval_metrics: eval_metrics.pop(\"uid\")\n",
    "                if \"question\" in eval_metrics: eval_metrics.pop(\"question\")\n",
    "\n",
    "                logger.log({\"steps\": global_steps, **eval_metrics},is_train=False)\n",
    "                global_pbar.write(f\"====== evaluation {global_steps} ====\")\n",
    "                for k, v in eval_metrics.items():\n",
    "                    global_pbar.write(f\"   {k}: {v}\")\n",
    "                global_pbar.write(f\"======================\")\n",
    "                checkpoint = f\"step-{global_steps}\"\n",
    "\n",
    "                best_models.append((checkpoint, eval_metrics[train_config.metric_for_best_model]))\n",
    "                best_models.sort(key=lambda x: x[1], reverse=train_config.metric_greater_is_better)\n",
    "                if len(best_models) > train_config.save_total_limit:\n",
    "                    need_to_remove = best_models.pop()\n",
    "                    if need_to_remove != checkpoint:\n",
    "                        logger.log({\"steps\": global_steps, \"remove_checkpoint\": need_to_remove[0]})\n",
    "                        shutil.rmtree(os.path.join(train_config.output_path, need_to_remove[0]), ignore_errors=True)\n",
    "                        logger.log({\"steps\": global_steps, \"save_chechkpoint\": checkpoint})\n",
    "                        os.makedirs(os.path.join(train_config.output_path, checkpoint), exist_ok=True)\n",
    "                        train_model.save_pretrained(os.path.join(train_config.output_path, checkpoint))\n",
    "                        tokenizer.save_pretrained(os.path.join(train_config.output_path, checkpoint))\n",
    "                else:\n",
    "                    logger.log({\"steps\": global_steps, \"save_chechkpoint\": checkpoint})\n",
    "                    os.makedirs(os.path.join(train_config.output_path, checkpoint), exist_ok=True)\n",
    "                    train_model.save_pretrained(os.path.join(train_config.output_path, checkpoint))\n",
    "                    tokenizer.save_pretrained(os.path.join(train_config.output_path, checkpoint))\n",
    "                global_pbar.disable = False\n",
    "                global_pbar.refresh()\n",
    "            if train_config.max_steps is not None and global_steps >= train_config.max_steps:\n",
    "                break\n",
    "        if train_config.max_steps is not None and global_steps >= train_config.max_steps:\n",
    "            break\n",
    "\n",
    "    eval_metrics = eval_loop(model, eval_loader, train_config, tokenizer)\n",
    "    logger.log({\"steps\": global_steps, **eval_metrics},is_train=False)\n",
    "    global_pbar.write(f\"====== Training End ====\")\n",
    "\n",
    "    if \"prediction\" in eval_metrics: eval_metrics.pop(\"prediction\")\n",
    "    if \"answers\" in eval_metrics: eval_metrics.pop(\"answers\")\n",
    "    if \"uid\" in eval_metrics: eval_metrics.pop(\"uid\")\n",
    "    if \"question\" in eval_metrics: eval_metrics.pop(\"question\")\n",
    "    for k, v in eval_metrics.items():\n",
    "        global_pbar.write(f\"   {k}: {v}\")\n",
    "    global_pbar.write(f\"======================\")\n",
    "    checkpoint = f\"step-{global_steps}\"\n",
    "\n",
    "    best_models.append((checkpoint, eval_metrics[train_config.metric_for_best_model]))\n",
    "    best_models.sort(key=lambda x: x[1], reverse=train_config.metric_greater_is_better)\n",
    "    if best_models[0][0] != checkpoint:\n",
    "        shutil.copytree(\n",
    "            os.path.join(train_config.output_path, best_models[0][0]),\n",
    "            os.path.join(train_config.output_path, \"best_model\"),\n",
    "            dirs_exist_ok=True,\n",
    "        )\n",
    "    else:\n",
    "        train_model.save_pretrained(os.path.join(train_config.output_path, \"best_model\"))\n",
    "        tokenizer.save_pretrained(os.path.join(train_config.output_path, \"best_model\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799,
     "referenced_widgets": [
      "5c8b1436a3bb41d08281ae2712eec063",
      "b5e0b7b4efbb44b5aced06fb1882ca62",
      "0e36a466d1f84e3484a159a139769062",
      "253e1cb6466749a99a1af7fcf049afb3",
      "3f9252fdc8984cf79abf54cc5d3f9ccf",
      "959955c6a2314333ad1a8950a5bbd07a",
      "a5147c81b7354b52984147b157fe560d",
      "9b952e7a9b964b469ebfc3bd3a0ec6ad",
      "d0fbec47818942d1a89ea4e84fe8cd65",
      "cfa1b9f20c4f4a01bc53b370ea422919",
      "04474e83860e4c01a10892961242b643",
      "14f0cf5723f94ce2a54127fc8cc0f914",
      "f915f3c4f5df4a3680ed1aa2d770597d",
      "b064703010da4e9ab933b2e5b34d297a",
      "010a83eb49dd44ea8a950f15e0f71e60",
      "3b94817d5e5d406e990070c3cc42c74b",
      "d97238196deb4400b10f007adff0ed05",
      "536df2098f9b4077a82c77262e223b64",
      "395b126e4368481c9b09e90664019a07",
      "ca900706746546c198f9894f1109835c",
      "daa478204caf4a74b85c0912a9c7457e",
      "b2011c1e226f4f4ea82b46b1e55d0487",
      "5b22f7f1bdec468a893d1252fb74329b",
      "0bea25eea440436cb217d897cdda46f6",
      "1b3ea0b85a274c1187603322a63b5cc8",
      "adcb2ec2de8c4576ac413b1029a36c16",
      "c81876b78b174ded8e7ac565c3dcd32c",
      "fbb12b2e38d0446186b3f23a37dd7580",
      "7da0e361ae50414badbdd5598e3c4425",
      "f72d7727c6d14b64916873e603b456be",
      "10ae8e76f7c24459b08b5d2d2b635688",
      "64f11ec2b1a24845b6da1bc691767ccc",
      "e6d0e517ff5a408ebd3701de19bdb043",
      "94c122b8f58948fbb715d7be1d568aad",
      "cd71ffd73bf34584ae7003d110b61f0c",
      "c21233b21b0d4c0293e16ef8f26e567b",
      "0410021dd1074dcb97f4cebac142019e",
      "8bb5dce01a1d4c2ba0f44e31c6b7e1c4",
      "5adcded64bb04bc4a1f612558a3b06bd",
      "db59ff6f27364aacb8e56a664d8584ea",
      "40d6667a9d244f8080d167635d906c16",
      "d4a6b6bf301c4bc583ce4d5af2ee8c07",
      "1a0abe34d0724bcc99a9736e6cc3fef5",
      "6a36e0397a0a4bd5a658670c30705a0c",
      "47cfbe08701b4a8f8a9ebaf8cf2ccfed",
      "228762a0892b4175aa98d3351802e491",
      "15794cc3490d43c0b513e05a25ec1c17",
      "a51b76dc477d49d8a1b3d23b0d062ead",
      "9ce17c3bef44415bba0b30c6a3b5dcb7",
      "fa12c0684d214019a8b7cc3b86d64daa",
      "40ffd473104e4613ae67974e779a8455",
      "75ea11b89d5c489e87bba1d661ac1114",
      "7f34b3ae4dae4ed6a1543190eff71a5c",
      "5d151df99fda4ae9963effdb64ba2f5b",
      "c2fb365b4348435493f782bf957c6f76",
      "1820744ef80b4fe4b4c25a3bbcf2adc8",
      "2ad544a8ceca4c0ba4442beea11422f1",
      "c99fd48965164a0086a824bde121b038",
      "8e05c846028b449ca9f9deef6adacd07",
      "5a9bab42b84d46f6aa229f13b31bccb7",
      "ac4569c968764541b5fe52401ff89710",
      "0fc0a19ce36a4f568a28e37beb61398e",
      "b37e0c6f2db841bca4847d8b73be7b54",
      "fd4ea03c73e745cca8e4e5d9447266be",
      "1ce6926c6f5e44f98ac15fbe927611be",
      "2fb8d1ee9b474cfeafbd5890806b12ba",
      "3b6db537ab1540bc8e6a652d9873f1fe",
      "45780dbe87f04aef8513c0762908b193",
      "80bfbe62f733467f9c886cdd9745c1af",
      "c0e619bbbcbc44049003cbbfba2949da",
      "0b8cc575827741c9bb5af3bd81f245a5",
      "97ac312adfff4c358bdcc04217b089b9",
      "484698dce69c45218234bf91a12a8bde",
      "858ab8abb08749f9a9de3d2bc2392c71",
      "b10f63b7976045fcb70cc6349333c037",
      "404321f63ebf497180d164b45d72965a",
      "ff5c6170ca08429094c3d2a868acd7d0",
      "32fdfac1882b42a3bea99f8d5d48ea33",
      "b69db0da8c7a42c9942eebd140724305",
      "9bf31c36ea034ad689c5ff2d91cc7e7b",
      "1aefc69eb99a47328a4adec7cdb8b8c4",
      "329914c1bc9045bd97852ecc9a74476e",
      "28260fa3f6fb42eea5ad7857b10ac396",
      "7d86bbcd499a4faeabfffc3fe0bc1d47",
      "8c54a104f9684547adf74275223d1c63",
      "24bf3e46bdbd47e09197f2ee15c55e9f",
      "c66d7a601bff4ae788800f69ea437e24",
      "de44fd480b4f41bca98564310c506249",
      "a33d26daec5c4b99be3f6ef0dbc973db",
      "7c35abbb646649288173a81a8e6b03df",
      "e135c8db8d67429eb936ce3d144d87df",
      "ce8f3896f9354add8e9ac0c72e0aef40",
      "4417e1f8aed74346b6b4d1a2167ce05b",
      "29840492a5644c5385a444dae097bfae",
      "fbb9f0d02df44621825f9efafd59bcc9",
      "ef996e77e5c54c25a66fa5df89a7aff9",
      "4de459958b1043dfacb0df5085a86474",
      "612b0e3d5c2949ed811cfd7b4dea6a78",
      "e325ffd30eba46919eb67fc1931199c9"
     ]
    },
    "id": "2Tgpn6hPhuU-",
    "outputId": "3aa1bbbd-fb68-4d63-b276-4ad506a99bb4"
   },
   "outputs": [],
   "source": [
    "# DO_PRETRAIN\n",
    "\n",
    "if DO_PRETRAIN:\n",
    "    ### About 350M parameters\n",
    "    # model_config = TransformerConfig(\n",
    "    #     vocab_size=len(tokenizer),\n",
    "    #     hidden_size=1024,\n",
    "    #     intermediate_size=4096,\n",
    "    #     num_hidden_layers=24,\n",
    "    #     num_attention_heads=16,\n",
    "    #     num_key_value_heads=4,\n",
    "    #     head_dim=64,\n",
    "    #     max_postion_embeddings=MODEL_CONTEXT_LENGTH,\n",
    "    #     attention_dropout=0.1,\n",
    "    #     ffn_dropout=0.05,\n",
    "    #     pad_token_id=tokenizer.pad_token_id,\n",
    "    #     bos_token_id=tokenizer.bos_token_id,\n",
    "    #     eos_token_id=tokenizer.eos_token_id,\n",
    "    #     rope_theta=ROPE_THETA,\n",
    "    # )\n",
    "\n",
    "    ### About 120M parameters\n",
    "    # model_config = TransformerConfig(\n",
    "    #     vocab_size=len(tokenizer),\n",
    "    #     hidden_size=768,\n",
    "    #     intermediate_size=3072,\n",
    "    #     num_hidden_layers=12,\n",
    "    #     num_attention_heads=12,\n",
    "    #     num_key_value_heads=4,\n",
    "    #     head_dim=64,\n",
    "    #     max_postion_embeddings=MODEL_CONTEXT_LENGTH,\n",
    "    #     attention_dropout=0.1,\n",
    "    #     ffn_dropout=0.05,\n",
    "    #     pad_token_id=tokenizer.pad_token_id,\n",
    "    #     bos_token_id=tokenizer.bos_token_id,\n",
    "    #     eos_token_id=tokenizer.eos_token_id,\n",
    "    #     rope_theta=ROPE_THETA,\n",
    "    # )\n",
    "\n",
    "    ## About 70M parameters\n",
    "    model_config = TransformerConfig(\n",
    "        vocab_size=len(tokenizer),\n",
    "        hidden_size=512,\n",
    "        intermediate_size=2048,\n",
    "        num_hidden_layers=4,\n",
    "        num_attention_heads=16,\n",
    "        num_key_value_heads=4,\n",
    "        head_dim=32,\n",
    "        max_postion_embeddings=MODEL_CONTEXT_LENGTH,\n",
    "        attention_dropout=0.1,\n",
    "        ffn_dropout=0.05,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        rope_theta=ROPE_THETA,\n",
    "    )\n",
    "\n",
    "    ## About 30M parameters\n",
    "    # model_config = TransformerConfig(\n",
    "    #     vocab_size=len(tokenizer),\n",
    "    #     hidden_size=384,\n",
    "    #     intermediate_size=1536,\n",
    "    #     num_hidden_layers=4,\n",
    "    #     num_attention_heads=4,\n",
    "    #     num_key_value_heads=2,\n",
    "    #     head_dim=32,\n",
    "    #     max_postion_embeddings=MODEL_CONTEXT_LENGTH,\n",
    "    #     attention_dropout=0.1,\n",
    "    #     ffn_dropout=0.05,\n",
    "    #     pad_token_id=tokenizer.pad_token_id,\n",
    "    #     bos_token_id=tokenizer.bos_token_id,\n",
    "    #     eos_token_id=tokenizer.eos_token_id,\n",
    "    #     rope_theta=ROPE_THETA,\n",
    "    # )\n",
    "\n",
    "\n",
    "    from dataset.pretrain import prepare_pretrain_dataset, collate_fn_for_pretrain\n",
    "\n",
    "    pretraining_config = TrainingConfig(\n",
    "        device=\"cuda\",\n",
    "        batch_size=16,\n",
    "        eval_batch_size=16,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=2,\n",
    "        max_steps=None,\n",
    "        optimizer_type=\"adamw\",\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=500,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        metric_for_best_model=\"ppl\",\n",
    "        eval_interval=5000,\n",
    "        logging_interval=10,\n",
    "        save_total_limit=3,\n",
    "        logging_path=os.path.join(LOG_PATH, \"pretraining\"),\n",
    "        output_path=os.path.join(OUTPUT_PATH, \"pretraining\"),\n",
    "    )\n",
    "\n",
    "    pretrain_train, pretrain_eval = prepare_pretrain_dataset(tokenizer, max_length=MODEL_CONTEXT_LENGTH, train_sample_size=1048576, eval_sample_size=8096, cache_path=DRIVE_CACHE_PATH)\n",
    "    model = TransformerForCausalLM(model_config)\n",
    "    collate_fn = functools.partial(collate_fn_for_pretrain,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    block_length=MODEL_CONTEXT_LENGTH)\n",
    "    train(model,\n",
    "        train_dataset=pretrain_train,\n",
    "        eval_dataset=pretrain_eval,\n",
    "        train_collate_fn=collate_fn,\n",
    "        eval_collate_fn=collate_fn,\n",
    "        train_config=pretraining_config,\n",
    "        eval_loop=eval_loop_for_pretraining)\n",
    "    del pretrain_train, pretrain_eval, model\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uO_2mfnVhuU_"
   },
   "outputs": [],
   "source": [
    "#FINETUNE_SM\n",
    "\n",
    "if DO_FINETUNE_SM:\n",
    "    from dataset.summary import prepare_summary_dataset, collate_fn_for_summary\n",
    "\n",
    "    summary_config = TrainingConfig(\n",
    "        device=\"cuda\",\n",
    "        batch_size=16,\n",
    "        eval_batch_size=16,\n",
    "        gradient_accumulation_steps=1,\n",
    "        num_train_epochs=3,\n",
    "        max_steps=None,\n",
    "        optimizer_type=\"adamw\",\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=500,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        metric_for_best_model=\"loss\",\n",
    "        eval_interval=5000,\n",
    "        logging_interval=10,\n",
    "        save_total_limit=3,\n",
    "        logging_path=os.path.join(LOG_PATH, \"summary\"),\n",
    "        output_path=os.path.join(OUTPUT_PATH, \"summary\"),\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join(OUTPUT_PATH, \"pretraining\", \"best_model\")\n",
    "    model = TransformerForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "    summary_train, summary_eval = prepare_summary_dataset(tokenizer)\n",
    "    collate_fn = functools.partial(collate_fn_for_summary,\n",
    "                                    tokenizer=tokenizer)\n",
    "    train(model,\n",
    "        train_dataset=summary_train,\n",
    "        eval_dataset=summary_eval,\n",
    "        train_collate_fn= collate_fn,\n",
    "        eval_collate_fn=collate_fn,\n",
    "        train_config=summary_config,\n",
    "        eval_loop=eval_loop_for_loss)\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    model = TransformerForCausalLM.from_pretrained(os.path.join(OUTPUT_PATH, \"summary\", \"best_model\"))\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        summary_eval,\n",
    "        batch_size=summary_config.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    metrics = eval_for_summary(model, eval_loader, summary_config, tokenizer, max_new_tokens=128)\n",
    "    prediction = metrics.pop(\"prediction\")\n",
    "    answers = metrics.pop(\"answers\")\n",
    "    with open(os.path.join(RESULTS_PATH,\"summary_score.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    with open(os.path.join(RESULTS_PATH,\"summary_output.txt\"), \"w\") as f:\n",
    "        for p,a in zip(prediction, answers):\n",
    "            a = a.replace(\"\\n\", \" \").strip()\n",
    "            p = p.replace(\"\\n\", \" \").strip()\n",
    "            f.write(f\"{a}\\t{p}\\n\")\n",
    "\n",
    "    del summary_train, summary_eval, model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataset.summary import prepare_summary_dataset, collate_fn_for_summary\n",
    "\n",
    "# summary_config = TrainingConfig(\n",
    "#     device=\"cuda\",\n",
    "#     batch_size=16,\n",
    "#     eval_batch_size=16,\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     num_train_epochs=3,\n",
    "#     max_steps=None,\n",
    "#     optimizer_type=\"adamw\",\n",
    "#     learning_rate=5e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     warmup_steps=500,\n",
    "#     max_grad_norm=1.0,\n",
    "#     lr_scheduler_type=\"cosine\",\n",
    "#     metric_for_best_model=\"loss\",\n",
    "#     eval_interval=5000,\n",
    "#     logging_interval=10,\n",
    "#     save_total_limit=3,\n",
    "#     logging_path=os.path.join(LOG_PATH, \"summary\"),\n",
    "#     output_path=os.path.join(OUTPUT_PATH, \"summary\"),\n",
    "# )\n",
    "\n",
    "# summary_train, summary_eval = prepare_summary_dataset(tokenizer)\n",
    "# collate_fn = functools.partial(collate_fn_for_summary,\n",
    "#                                 tokenizer=tokenizer)\n",
    "# model = TransformerForCausalLM.from_pretrained(os.path.join(OUTPUT_PATH, \"summary\", \"best_model\"))\n",
    "# eval_loader = torch.utils.data.DataLoader(\n",
    "#     summary_eval,\n",
    "#     batch_size=summary_config.eval_batch_size,\n",
    "#     shuffle=False,\n",
    "#     collate_fn=collate_fn,\n",
    "# )\n",
    "# metrics = eval_for_summary(model, eval_loader, summary_config, tokenizer, max_new_tokens=128)\n",
    "# prediction = metrics.pop(\"prediction\")\n",
    "# answers = metrics.pop(\"answers\")\n",
    "# with open(os.path.join(RESULTS_PATH,\"summary_score.json\"), \"w\") as f:\n",
    "#     json.dump(metrics, f, indent=4)\n",
    "# with open(os.path.join(RESULTS_PATH,\"summary_output.txt\"), \"w\") as f:\n",
    "#     for p,a in zip(prediction, answers):\n",
    "#         a = a.replace(\"\\n\", \" \").strip()\n",
    "#         p = p.replace(\"\\n\", \" \").strip()\n",
    "#         f.write(f\"{a}\\t{p}\\n\")\n",
    "\n",
    "# del summary_train, summary_eval, model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nA1Wzf3YhuU_"
   },
   "outputs": [],
   "source": [
    "# FINETUNE_CF\n",
    "\n",
    "if DO_FINETUNE_CF:\n",
    "    from dataset.classification import prepare_classification_dataset, collate_fn_for_classification\n",
    "\n",
    "    classification_config = TrainingConfig(\n",
    "        device=\"cuda\",\n",
    "        batch_size=32,\n",
    "        eval_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        num_train_epochs=5,\n",
    "        max_steps=None,\n",
    "        optimizer_type=\"adamw\",\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=200,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        metric_for_best_model=\"loss\",\n",
    "        eval_interval=2500,\n",
    "        logging_interval=10,\n",
    "        save_total_limit=3,\n",
    "        logging_path=os.path.join(LOG_PATH, \"classification\"),\n",
    "        output_path=os.path.join(OUTPUT_PATH, \"classification\"),\n",
    "    )\n",
    "\n",
    "    model = TransformerForSequenceClassification.from_pretrained(os.path.join(OUTPUT_PATH, \"pretraining\", \"best_model\"),num_labels=20)\n",
    "    class_train, class_eval = prepare_classification_dataset(tokenizer)\n",
    "    collate_fn = functools.partial(collate_fn_for_classification,\n",
    "                                    tokenizer=tokenizer)\n",
    "    train(model,\n",
    "        train_dataset=class_train,\n",
    "        eval_dataset=class_eval,\n",
    "        train_collate_fn=collate_fn,\n",
    "        eval_collate_fn=collate_fn,\n",
    "        train_config=classification_config,\n",
    "        eval_loop=eval_loop_for_loss)\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    model = TransformerForSequenceClassification.from_pretrained(os.path.join(OUTPUT_PATH, \"classification\", \"best_model\"),num_labels=20)\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        class_eval,\n",
    "        batch_size=classification_config.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    metrics = eval_for_classification(model, eval_loader, classification_config)\n",
    "    prediction = metrics.pop(\"prediction\")\n",
    "    answers = metrics.pop(\"answers\")\n",
    "    with open(os.path.join(RESULTS_PATH,\"classification_score.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    with open(os.path.join(RESULTS_PATH,\"classification_output.txt\"), \"w\") as f:\n",
    "        for p,a in zip(prediction, answers):\n",
    "            f.write(f\"{a}\\t{p}\\n\")\n",
    "\n",
    "    del class_train, class_eval, model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/06/2025 13:00:37 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "loading configuration file /mountpoint/lkj/NLP/output/pretraining/best_model/config.json\n",
      "Model config TransformerConfig {\n",
      "  \"architectures\": [\n",
      "    \"TransformerForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"ffn_dropout\": 0.05,\n",
      "  \"head_dim\": 32,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"custom_transformer\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 20000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"vocab_size\": 50258\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading weights file /mountpoint/lkj/NLP/output/pretraining/best_model/model.safetensors\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at /mountpoint/lkj/NLP/output/pretraining/best_model were not used when initializing TransformerForCausalLM: ['model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.bias']\n",
      "- This IS expected if you are initializing TransformerForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TransformerForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TransformerForCausalLM were not initialized from the model checkpoint at /mountpoint/lkj/NLP/output/pretraining/best_model and are newly initialized: ['model.layers.0.self_attn.out_proj.weight', 'model.layers.1.self_attn.out_proj.weight', 'model.layers.2.self_attn.out_proj.weight', 'model.layers.3.self_attn.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50266. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "06/06/2025 13:00:38 - INFO - __main__ - Initializing LORA model...\n",
      "trainable params: 425,984 || all params: 67,104,256 || trainable%: 0.6348\n",
      "Tokenizing and reformatting instruction data: 100%|â–ˆ| 43685/43685 [00:49<00:00, \n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43685/43685 [00:01<00:00, 27293.04 examples/s]\n",
      "06/06/2025 13:02:04 - INFO - __main__ - ***** Running training *****\n",
      "06/06/2025 13:02:04 - INFO - __main__ -   Num examples = 43685\n",
      "06/06/2025 13:02:04 - INFO - __main__ -   Num Epochs = 3\n",
      "06/06/2025 13:02:04 - INFO - __main__ -   Instantaneous batch size per device = 4\n",
      "06/06/2025 13:02:04 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "06/06/2025 13:02:04 - INFO - __main__ -   Gradient Accumulation steps = 2\n",
      "06/06/2025 13:02:04 - INFO - __main__ -   Total optimization steps = 16383\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16383/16383 [20:14<00:00, 13.11it/s]tokenizer config file saved in /mountpoint/lkj/NLP/output/pre_lora_finetuned/tokenizer_config.json\n",
      "Special tokens file saved in /mountpoint/lkj/NLP/output/pre_lora_finetuned/special_tokens_map.json\n",
      "loading configuration file /mountpoint/lkj/NLP/output/pretraining/best_model/config.json\n",
      "Model config TransformerConfig {\n",
      "  \"architectures\": [\n",
      "    \"TransformerForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"ffn_dropout\": 0.05,\n",
      "  \"head_dim\": 32,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"custom_transformer\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 20000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"vocab_size\": 50258\n",
      "}\n",
      "\n",
      "/home/guest2/anaconda3/envs/kangjun/lib/python3.12/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16383/16383 [20:14<00:00, 13.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Finetune Pretrained model before RAG finetuning.\n",
    "if DO_PRE_FINETUNED_RAG:\n",
    "    !python finetune.py   --train_file train.jsonl   --model_name_or_path /mountpoint/lkj/NLP/output/pretraining/best_model   --tokenizer_name /mountpoint/lkj/NLP/output/pretraining/best_model   --output_dir /mountpoint/lkj/NLP/output/pre_lora_finetuned  --use_lora   --use_special_tokens   --num_train_epochs 3   --per_device_train_batch_size 4   --gradient_accumulation_steps 2   --learning_rate 5e-5   --max_seq_length 512   --overwrite_cache   --low_cpu_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at /mountpoint/lkj/NLP/output/pretraining/best_model were not used when initializing TransformerForCausalLM: ['model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.bias']\n",
      "- This IS expected if you are initializing TransformerForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TransformerForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TransformerForCausalLM were not initialized from the model checkpoint at /mountpoint/lkj/NLP/output/pretraining/best_model and are newly initialized: ['model.layers.0.self_attn.out_proj.weight', 'model.layers.1.self_attn.out_proj.weight', 'model.layers.2.self_attn.out_proj.weight', 'model.layers.3.self_attn.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at 'local_cache/rag/data/nq_open_dpr/nq_dev'\n",
      "Dataset already exists at 'local_cache/rag/data/nq_open_dpr/nq_train'\n",
      "Training Total size=63.59M params. Trainable ratio=100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b770d874bed4695b96c81bb236490c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa859b1738d4d86aacb234986735759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== evaluation 2000 ====\n",
      "   accuracy: 0.0\n",
      "   rouge1: 0.011095085310357768\n",
      "   rouge2: 0.0\n",
      "   rougeL: 0.011073950352538237\n",
      "   rougeLsum: 0.011118963964320841\n",
      "======================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207f45a36ead47338750a1b2adf6aa24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== evaluation 4000 ====\n",
      "   accuracy: 0.0\n",
      "   rouge1: 0.010342532882824527\n",
      "   rouge2: 0.0\n",
      "   rougeL: 0.010341153474307741\n",
      "   rougeLsum: 0.010362743549619998\n",
      "======================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad81327dc1d41e984096284d6a26933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== evaluation 6000 ====\n",
      "   accuracy: 0.0\n",
      "   rouge1: 0.010330713032171217\n",
      "   rouge2: 0.0\n",
      "   rougeL: 0.010326636142062091\n",
      "   rougeLsum: 0.010347092150622478\n",
      "======================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cf50cebdea4269b2029e94a8bb85c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== evaluation 8000 ====\n",
      "   accuracy: 0.0\n",
      "   rouge1: 0.01032789224078557\n",
      "   rouge2: 0.0\n",
      "   rougeL: 0.010324905332579935\n",
      "   rougeLsum: 0.010345954250405527\n",
      "======================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef69edceb54409993d4b362ea0c8e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== evaluation 10000 ====\n",
      "   accuracy: 0.0\n",
      "   rouge1: 0.01032789224078557\n",
      "   rouge2: 0.0\n",
      "   rougeL: 0.010324905332579935\n",
      "   rougeLsum: 0.010345954250405527\n",
      "======================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6c19fc96b54129a3c20566494496cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Training End ====\n",
      "   accuracy: 0.0\n",
      "   rouge1: 0.01032789224078557\n",
      "   rouge2: 0.0\n",
      "   rougeL: 0.010324905332579935\n",
      "   rougeLsum: 0.010345954250405527\n",
      "======================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e92a68a713402e8a984da6fcab1a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== evaluation ====\n",
      "   accuracy: 0.0\n",
      "   rouge1: 0.011095085310357768\n",
      "   rouge2: 0.0\n",
      "   rougeL: 0.011073950352538237\n",
      "   rougeLsum: 0.011118963964320841\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "# PRE_FINETUNED_RAG\n",
    "# RAG Finetuning on LoRA finetuned model\n",
    "if DO_PRE_FINETUNED_RAG:\n",
    "    from dataset.rag import donwload_dataset_nq_open_dpr, RAGDataset, RAGCollator\n",
    "    from pyserini.search.lucene import LuceneSearcher\n",
    "    from model_rag import ModelRAG\n",
    "    from model_lora import TransformerForCausalLM, TransformerConfig\n",
    "\n",
    "    rag_config = TrainingConfig(\n",
    "        device=\"cuda\",\n",
    "        batch_size=16,\n",
    "        eval_batch_size=16,\n",
    "        gradient_accumulation_steps=1,\n",
    "        num_train_epochs=3,\n",
    "        max_steps=None,\n",
    "        optimizer_type=\"adamw\",\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=300,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        metric_greater_is_better=True,\n",
    "        eval_interval=2000,\n",
    "        logging_interval=10,\n",
    "        save_total_limit=3,\n",
    "        logging_path=os.path.join(LOG_PATH, \"pre_finetuned_rag\"),\n",
    "        output_path=os.path.join(OUTPUT_PATH, \"pre_finetuned_rag\"),\n",
    "    )\n",
    "    assert os.path.isdir(\"/mountpoint/lkj/NLP/output/pre_lora_finetuned\")\n",
    "    tokenizer_lora = transformers.AutoTokenizer.from_pretrained(\"/mountpoint/lkj/NLP/output/pre_lora_finetuned\")\n",
    "\n",
    "    rag_cache_path = os.path.join(LOCAL_CACHE_PATH, \"rag\")\n",
    "    donwload_dataset_nq_open_dpr(rag_cache_path)\n",
    "\n",
    "    PREBUILT_INDEX_NAME_BM25 = \"wikipedia-dpr-100w\"\n",
    "    LOCAL_INDEX_NAME_BM25 = os.path.join(rag_cache_path, \"lucene-index.wikipedia-dpr-100w.20210120.d1b9e6\")\n",
    "    # it might take a 10-20 minutes to download the index, recommand to use drive cache, if drive capacity is enough\n",
    "\n",
    "    try:\n",
    "        searcher_bm25 = LuceneSearcher(index_dir=LOCAL_INDEX_NAME_BM25)\n",
    "    except:\n",
    "        import shutil\n",
    "        searcher_bm25 = LuceneSearcher.from_prebuilt_index(prebuilt_index_name=PREBUILT_INDEX_NAME_BM25)\n",
    "        index_dir = searcher_bm25.index_dir\n",
    "        shutil.move(index_dir, LOCAL_INDEX_NAME_BM25)\n",
    "        searcher_bm25 = LuceneSearcher(index_dir=LOCAL_INDEX_NAME_BM25)\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    from peft import PeftModel, PeftConfig\n",
    "\n",
    "    OUTPUT_PATH_2 = \"/mountpoint/lkj/NLP/output/pre_lora_finetuned\"\n",
    "\n",
    "    peft_config = PeftConfig.from_pretrained(OUTPUT_PATH_2)\n",
    "\n",
    "    base_model = TransformerForCausalLM.from_pretrained(\n",
    "        peft_config.base_model_name_or_path,  \n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(OUTPUT_PATH_2)\n",
    "\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    train_dataset = RAGDataset(\n",
    "        tokenizer=tokenizer_lora,\n",
    "        is_train=True,\n",
    "        dataset_path=os.path.join(rag_cache_path,\"data\",\"nq_open_dpr\",\"nq_train\"),\n",
    "        num_samples=None\n",
    "    )\n",
    "    eval_dataset = RAGDataset(\n",
    "        tokenizer=tokenizer_lora,\n",
    "        is_train=False,\n",
    "        dataset_path=os.path.join(rag_cache_path,\"data\",\"nq_open_dpr\",\"nq_dev\"),\n",
    "    )\n",
    "    train_collator = RAGCollator(\n",
    "        tokenizer=tokenizer_lora,\n",
    "        is_train=True,\n",
    "    )\n",
    "    eval_collator = RAGCollator(\n",
    "        tokenizer=tokenizer_lora,\n",
    "        is_train=False,\n",
    "    )\n",
    "\n",
    "    model_rag = ModelRAG()\n",
    "    model_rag.set_model(model)\n",
    "    model_rag.set_tokenizer(tokenizer_lora)\n",
    "    model_rag.set_retriever(searcher_bm25)\n",
    "\n",
    "\n",
    "    train(\n",
    "        model_rag,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        train_collate_fn=train_collator,\n",
    "        eval_collate_fn=eval_collator,\n",
    "        train_config=rag_config,\n",
    "        eval_loop=eval_for_rag,\n",
    "    )\n",
    "\n",
    "    model_rag.set_model(None)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    model = TransformerForCausalLM.from_pretrained(os.path.join(OUTPUT_PATH, \"pre_finetuned_rag\", \"best_model\"))\n",
    "\n",
    "    model_rag.set_model(model)\n",
    "\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=rag_config.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=eval_collator,\n",
    "    )\n",
    "    metrics = eval_for_rag(model_rag, eval_loader, rag_config, tokenizer_lora)\n",
    "    prediction = metrics.pop(\"prediction\")\n",
    "    answers = metrics.pop(\"answers\")\n",
    "    uid = metrics.pop(\"uid\")\n",
    "    questions = metrics.pop(\"question\")\n",
    "    \n",
    "    print(f\"====== evaluation ====\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"   {k}: {v}\")\n",
    "    print(f\"======================\")\n",
    "          \n",
    "    with open(os.path.join(RESULTS_PATH,\"pre_finetuned_rag_score.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    with open(os.path.join(RESULTS_PATH,\"pre_finetuned_rag_output.txt\"), \"w\") as f:\n",
    "        for u,q,p,a in zip(uid, questions, prediction, answers):\n",
    "            f.write(f\"{u}\\t{q}\\t{a}\\t{p}\\n\")\n",
    "\n",
    "    del train_dataset, eval_dataset, model, model_rag\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Llg_jIELhuU_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at /mountpoint/lkj/NLP/output/pretraining/best_model were not used when initializing TransformerForCausalLM: ['model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.bias']\n",
      "- This IS expected if you are initializing TransformerForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TransformerForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TransformerForCausalLM were not initialized from the model checkpoint at /mountpoint/lkj/NLP/output/pretraining/best_model and are newly initialized: ['model.layers.0.self_attn.out_proj.weight', 'model.layers.1.self_attn.out_proj.weight', 'model.layers.2.self_attn.out_proj.weight', 'model.layers.3.self_attn.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at 'local_cache/rag/data/nq_open_dpr/nq_dev'\n",
      "Dataset already exists at 'local_cache/rag/data/nq_open_dpr/nq_train'\n",
      "Training Total size=63.59M params. Trainable ratio=100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80d8bb4d3b6419e9d75eea3b83451f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6cbcd326fb4bcca1944eb3f4f5678b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== evaluation 2000 ====\n",
      "   accuracy: 0.0\n",
      "   rouge1: 7.674597083653108e-05\n",
      "   rouge2: 0.0\n",
      "   rougeL: 8.697876694806856e-05\n",
      "   rougeLsum: 7.674597083653108e-05\n",
      "======================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34bf916ad494c12a618b76a318814dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== evaluation 4000 ====\n",
      "   accuracy: 0.00015349194167306216\n",
      "   rouge1: 0.0006406080063025572\n",
      "   rouge2: 0.0\n",
      "   rougeL: 0.0006484272102077167\n",
      "   rougeLsum: 0.0006566231585418078\n",
      "======================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e7175de3f249c0bf0b8d9c879e668c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== evaluation 6000 ====\n",
      "   accuracy: 0.00015349194167306216\n",
      "   rouge1: 0.00026110497330758265\n",
      "   rouge2: 0.0\n",
      "   rougeL: 0.00026110497330758265\n",
      "   rougeLsum: 0.00026256680132351654\n",
      "======================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3665626aa9ac4fc4b4156cdb8a8895a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== evaluation 8000 ====\n",
      "   accuracy: 0.00015349194167306216\n",
      "   rouge1: 0.00026110497330758265\n",
      "   rouge2: 0.0\n",
      "   rougeL: 0.00026110497330758265\n",
      "   rougeLsum: 0.00026256680132351654\n",
      "======================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3c96f88bc44fea81767efb72931fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== evaluation 10000 ====\n",
      "   accuracy: 0.00015349194167306216\n",
      "   rouge1: 0.00026110497330758265\n",
      "   rouge2: 0.0\n",
      "   rougeL: 0.00026110497330758265\n",
      "   rougeLsum: 0.00026256680132351654\n",
      "======================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdae90068e8443deb9e47efca6182af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Training End ====\n",
      "   accuracy: 0.00015349194167306216\n",
      "   rouge1: 0.00026110497330758265\n",
      "   rouge2: 0.0\n",
      "   rougeL: 0.00026110497330758265\n",
      "   rougeLsum: 0.00026256680132351654\n",
      "======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8af34245d3749da9a9b92d9216685ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== evaluation ====\n",
      "   accuracy: 0.00015349194167306216\n",
      "   rouge1: 0.0006406080063025572\n",
      "   rouge2: 0.0\n",
      "   rougeL: 0.0006484272102077167\n",
      "   rougeLsum: 0.0006566231585418078\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "# FINETUNE_RAG\n",
    "\n",
    "if DO_FINETUNE_RAG:\n",
    "    from dataset.rag import donwload_dataset_nq_open_dpr, RAGDataset, RAGCollator\n",
    "    from pyserini.search.lucene import LuceneSearcher\n",
    "    from model_rag import ModelRAG\n",
    "\n",
    "    rag_config = TrainingConfig(\n",
    "        device=\"cuda\",\n",
    "        batch_size=16,\n",
    "        eval_batch_size=16,\n",
    "        gradient_accumulation_steps=1,\n",
    "        num_train_epochs=3,\n",
    "        max_steps=None,\n",
    "        optimizer_type=\"adamw\",\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=300,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        metric_greater_is_better=True,\n",
    "        eval_interval=2000,\n",
    "        logging_interval=10,\n",
    "        save_total_limit=3,\n",
    "        logging_path=os.path.join(LOG_PATH, \"rag\"),\n",
    "        output_path=os.path.join(OUTPUT_PATH, \"rag\"),\n",
    "    )\n",
    "\n",
    "    rag_cache_path = os.path.join(LOCAL_CACHE_PATH, \"rag\")\n",
    "    donwload_dataset_nq_open_dpr(rag_cache_path)\n",
    "\n",
    "    PREBUILT_INDEX_NAME_BM25 = \"wikipedia-dpr-100w\"\n",
    "    LOCAL_INDEX_NAME_BM25 = os.path.join(rag_cache_path, \"lucene-index.wikipedia-dpr-100w.20210120.d1b9e6\")\n",
    "    # it might take a 10-20 minutes to download the index, recommand to use drive cache, if drive capacity is enough\n",
    "\n",
    "    try:\n",
    "        searcher_bm25 = LuceneSearcher(index_dir=LOCAL_INDEX_NAME_BM25)\n",
    "    except:\n",
    "        import shutil\n",
    "        searcher_bm25 = LuceneSearcher.from_prebuilt_index(prebuilt_index_name=PREBUILT_INDEX_NAME_BM25)\n",
    "        index_dir = searcher_bm25.index_dir\n",
    "        shutil.move(index_dir, LOCAL_INDEX_NAME_BM25)\n",
    "        searcher_bm25 = LuceneSearcher(index_dir=LOCAL_INDEX_NAME_BM25)\n",
    "\n",
    "    model = TransformerForCausalLM.from_pretrained(os.path.join(OUTPUT_PATH, \"pretraining\", \"best_model\"))\n",
    "    train_dataset = RAGDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        is_train=True,\n",
    "        dataset_path=os.path.join(rag_cache_path,\"data\",\"nq_open_dpr\",\"nq_train\"),\n",
    "        num_samples=None\n",
    "    )\n",
    "    eval_dataset = RAGDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        is_train=False,\n",
    "        dataset_path=os.path.join(rag_cache_path,\"data\",\"nq_open_dpr\",\"nq_dev\"),\n",
    "    )\n",
    "    train_collator = RAGCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        is_train=True,\n",
    "    )\n",
    "    eval_collator = RAGCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        is_train=False,\n",
    "    )\n",
    "\n",
    "    model_rag = ModelRAG()\n",
    "    model_rag.set_model(model)\n",
    "    model_rag.set_tokenizer(tokenizer)\n",
    "    model_rag.set_retriever(searcher_bm25)\n",
    "\n",
    "\n",
    "    train(\n",
    "        model_rag,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        train_collate_fn=train_collator,\n",
    "        eval_collate_fn=eval_collator,\n",
    "        train_config=rag_config,\n",
    "        eval_loop=eval_for_rag,\n",
    "    )\n",
    "\n",
    "    model_rag.set_model(None)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    model = TransformerForCausalLM.from_pretrained(os.path.join(OUTPUT_PATH, \"rag\", \"best_model\"))\n",
    "\n",
    "    model_rag.set_model(model)\n",
    "\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=rag_config.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=eval_collator,\n",
    "    )\n",
    "    metrics = eval_for_rag(model_rag, eval_loader, rag_config, tokenizer)\n",
    "    prediction = metrics.pop(\"prediction\")\n",
    "    answers = metrics.pop(\"answers\")\n",
    "    uid = metrics.pop(\"uid\")\n",
    "    questions = metrics.pop(\"question\")\n",
    "    \n",
    "    print(f\"====== evaluation ====\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"   {k}: {v}\")\n",
    "    print(f\"======================\")\n",
    "          \n",
    "    with open(os.path.join(RESULTS_PATH,\"rag_score.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    with open(os.path.join(RESULTS_PATH,\"rag_output.txt\"), \"w\") as f:\n",
    "        for u,q,p,a in zip(uid, questions, prediction, answers):\n",
    "            f.write(f\"{u}\\t{q}\\t{a}\\t{p}\\n\")\n",
    "\n",
    "    del train_dataset, eval_dataset, model, model_rag\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "u_cItmOykobj"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88c654eedea452d98a5e0e70f7ee7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8g59T_mhuU_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at 'local_cache/rag/data/nq_open_dpr/nq_dev'\n",
      "Dataset already exists at 'local_cache/rag/data/nq_open_dpr/nq_train'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4424c71a80ad47e9981bb19f71d1772e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1629 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:51:32.174000 2166137 site-packages/torch/_dynamo/convert_frame.py:844] [1/9] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0605 23:51:32.174000 2166137 site-packages/torch/_dynamo/convert_frame.py:844] [1/9]    function: 'wrapper' (/home/guest2/anaconda3/envs/kangjun/lib/python3.12/site-packages/transformers/utils/generic.py:953)\n",
      "W0605 23:51:32.174000 2166137 site-packages/torch/_dynamo/convert_frame.py:844] [1/9]    last reason: 1/0: tensor 'L['kwargs']['cache_position']' size mismatch at index 0. expected 56, actual 1\n",
      "W0605 23:51:32.174000 2166137 site-packages/torch/_dynamo/convert_frame.py:844] [1/9] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0605 23:51:32.174000 2166137 site-packages/torch/_dynamo/convert_frame.py:844] [1/9] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    }
   ],
   "source": [
    "# ZEROSHOT_RAG\n",
    "\n",
    "if DO_ZEROSHOT_RAG:\n",
    "    from dataset.rag import donwload_dataset_nq_open_dpr, RAGDataset, RAGCollator\n",
    "    from pyserini.search.lucene import LuceneSearcher\n",
    "    from model_rag import ModelRAG\n",
    "\n",
    "    rag_cache_path = os.path.join(LOCAL_CACHE_PATH, \"rag\")\n",
    "    donwload_dataset_nq_open_dpr(rag_cache_path)\n",
    "\n",
    "    PREBUILT_INDEX_NAME_BM25 = \"wikipedia-dpr-100w\"\n",
    "    LOCAL_INDEX_NAME_BM25 = os.path.join(rag_cache_path, \"lucene-index.wikipedia-dpr-100w.20210120.d1b9e6\")\n",
    "    # it might take a 10-20 minutes to download the index, recommand to use drive cache, if drive capacity is enough\n",
    "\n",
    "    try:\n",
    "        searcher_bm25 = LuceneSearcher(index_dir=LOCAL_INDEX_NAME_BM25)\n",
    "    except:\n",
    "        import shutil\n",
    "        searcher_bm25 = LuceneSearcher.from_prebuilt_index(prebuilt_index_name=PREBUILT_INDEX_NAME_BM25)\n",
    "        index_dir = searcher_bm25.index_dir\n",
    "        shutil.move(index_dir, LOCAL_INDEX_NAME_BM25)\n",
    "        searcher_bm25 = LuceneSearcher(index_dir=LOCAL_INDEX_NAME_BM25)\n",
    "\n",
    "\n",
    "    model_name_or_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    llm_model = transformers.AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                                                  torch_dtype=torch.bfloat16,\n",
    "                                                                  device_map={\"\": 0},\n",
    "                                                                  low_cpu_mem_usage=True,)\n",
    "    llm_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    llm_batch_size = 4\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "    llm_tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "    eval_dataset = RAGDataset(\n",
    "        tokenizer=llm_tokenizer,\n",
    "        is_train=False,\n",
    "        dataset_path=os.path.join(rag_cache_path,\"data\",\"nq_open_dpr\",\"nq_dev\"),\n",
    "    )\n",
    "    eval_collator = RAGCollator(\n",
    "        tokenizer=llm_tokenizer,\n",
    "        is_train=False,\n",
    "    )\n",
    "\n",
    "    model_rag = ModelRAG()\n",
    "    model_rag.set_model(llm_model)\n",
    "    model_rag.set_tokenizer(llm_tokenizer)\n",
    "    model_rag.set_retriever(searcher_bm25)\n",
    "\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=llm_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=eval_collator,\n",
    "    )\n",
    "    class rag_config:\n",
    "        device = \"cuda\"\n",
    "        model_dtype = \"cast_bf16\"\n",
    "\n",
    "    metrics = eval_for_rag(model_rag, eval_loader, rag_config, tokenizer)\n",
    "    prediction = metrics.pop(\"prediction\")\n",
    "    answers = metrics.pop(\"answers\")\n",
    "    uid = metrics.pop(\"uid\")\n",
    "    questions = metrics.pop(\"question\")\n",
    "    with open(os.path.join(RESULTS_PATH,\"llm_rag_score.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    with open(os.path.join(RESULTS_PATH,\"llm_rag_output.txt\"), \"w\") as f:\n",
    "        for u,q,p,a in zip(uid, questions, prediction, answers):\n",
    "            f.write(f\"{u}\\t{q}\\t{a}\\t{p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZEROSHOT_RAG\n",
    "# ZEROSHOT with my own custom Prompt\n",
    "\n",
    "if DO_ZEROSHOT_RAG_PROMPT:\n",
    "    from dataset.rag import donwload_dataset_nq_open_dpr, RAGDataset, RAGCollator\n",
    "    from pyserini.search.lucene import LuceneSearcher\n",
    "    from model_rag import ModelRAG\n",
    "    from types import MethodType\n",
    "\n",
    "    def custom_make_augmented_inputs(self, queries, qids, k=5):\n",
    "        list_passages, list_scores = self.search(queries, qids, k=k)\n",
    "        list_input_texts = []\n",
    "\n",
    "        for query, passages in zip(queries, list_passages):\n",
    "            document = \"\\n\\n\".join(passages)\n",
    "            prompt = (\n",
    "                \"<|begin_of_text|><|user|>\\n\"\n",
    "                \"DOCUMENT:\\n\"\n",
    "                f\"{document}\\n\\n\"\n",
    "                \"QUESTION:\\n\"\n",
    "                f\"{query}\\n\\n\"\n",
    "                \"INSTRUCTIONS:\\n\"\n",
    "                \"Answer the user's QUESTION using the DOCUMENT text above.\\n\"\n",
    "                \"Keep your answer grounded in the facts of the DOCUMENT.\\n\"\n",
    "                \"If the DOCUMENT doesnâ€™t contain the facts to answer the QUESTION, return {{NONE}}.\\n\"\n",
    "                \"<|end_of_text|><|assistant|>\\n\"\n",
    "            )\n",
    "            list_input_texts.append(prompt)\n",
    "\n",
    "        return list_input_texts\n",
    "\n",
    "    rag_cache_path = os.path.join(LOCAL_CACHE_PATH, \"rag\")\n",
    "    donwload_dataset_nq_open_dpr(rag_cache_path)\n",
    "\n",
    "    PREBUILT_INDEX_NAME_BM25 = \"wikipedia-dpr-100w\"\n",
    "    LOCAL_INDEX_NAME_BM25 = os.path.join(rag_cache_path, \"lucene-index.wikipedia-dpr-100w.20210120.d1b9e6\")\n",
    "    # it might take a 10-20 minutes to download the index, recommand to use drive cache, if drive capacity is enough\n",
    "\n",
    "    try:\n",
    "        searcher_bm25 = LuceneSearcher(index_dir=LOCAL_INDEX_NAME_BM25)\n",
    "    except:\n",
    "        import shutil\n",
    "        searcher_bm25 = LuceneSearcher.from_prebuilt_index(prebuilt_index_name=PREBUILT_INDEX_NAME_BM25)\n",
    "        index_dir = searcher_bm25.index_dir\n",
    "        shutil.move(index_dir, LOCAL_INDEX_NAME_BM25)\n",
    "        searcher_bm25 = LuceneSearcher(index_dir=LOCAL_INDEX_NAME_BM25)\n",
    "\n",
    "\n",
    "    model_name_or_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    llm_model = transformers.AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                                                  torch_dtype=torch.bfloat16,\n",
    "                                                                  device_map={\"\": 0},\n",
    "                                                                  low_cpu_mem_usage=True,)\n",
    "    llm_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    llm_batch_size = 4\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "    llm_tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "    eval_dataset = RAGDataset(\n",
    "        tokenizer=llm_tokenizer,\n",
    "        is_train=False,\n",
    "        dataset_path=os.path.join(rag_cache_path,\"data\",\"nq_open_dpr\",\"nq_dev\"),\n",
    "    )\n",
    "    eval_collator = RAGCollator(\n",
    "        tokenizer=llm_tokenizer,\n",
    "        is_train=False,\n",
    "    )\n",
    "\n",
    "    model_rag = ModelRAG()\n",
    "    model_rag.set_model(llm_model)\n",
    "    model_rag.set_tokenizer(llm_tokenizer)\n",
    "    model_rag.set_retriever(searcher_bm25)\n",
    "    model_rag.make_augmented_inputs_for_generate = MethodType(custom_make_augmented_inputs, model_rag)\n",
    "\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=llm_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=eval_collator,\n",
    "    )\n",
    "    class rag_config:\n",
    "        device = \"cuda\"\n",
    "        model_dtype = \"cast_bf16\"\n",
    "\n",
    "    metrics = eval_for_rag(model_rag, eval_loader, rag_config, tokenizer)\n",
    "    prediction = metrics.pop(\"prediction\")\n",
    "    answers = metrics.pop(\"answers\")\n",
    "    uid = metrics.pop(\"uid\")\n",
    "    questions = metrics.pop(\"question\")\n",
    "    with open(os.path.join(RESULTS_PATH,\"llm_prompt_rag_score.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    with open(os.path.join(RESULTS_PATH,\"llm_prompt_rag_output.txt\"), \"w\") as f:\n",
    "        for u,q,p,a in zip(uid, questions, prediction, answers):\n",
    "            f.write(f\"{u}\\t{q}\\t{a}\\t{p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZEROSHOT_RAG\n",
    "# ZEROSHOT with Chain of thought Prompt\n",
    "if DO_ZEROSHOT_RAG_PROMPT:\n",
    "    from dataset.rag import donwload_dataset_nq_open_dpr, RAGDataset, RAGCollator\n",
    "    from pyserini.search.lucene import LuceneSearcher\n",
    "    from model_rag import ModelRAG\n",
    "    from types import MethodType\n",
    "\n",
    "    def cot_make_augmented_inputs(self, queries, qids, k=5):\n",
    "        # Get the relevant documents for each query\n",
    "        list_passages, list_scores = self.search(queries, qids, k=k)\n",
    "        \n",
    "        list_input_text_without_answer = []\n",
    "        # fill here\n",
    "        ######\n",
    "        \n",
    "        for query, passages in zip(queries, list_passages):\n",
    "            context = \"\"\n",
    "            for passage in passages:\n",
    "                context += f\"Title: \\nPassage: {passage}\\n\"\n",
    "            \n",
    "            prompt = (\n",
    "                f\"{context}\"\n",
    "                f\"Question: {query}\\n\"\n",
    "                \"Let's think step by step.\"\n",
    "                \"Answer:\"\n",
    "            )\n",
    "            list_input_text_without_answer.append(prompt)\n",
    "        \n",
    "        ######\n",
    "        \n",
    "        return list_input_text_without_answer\n",
    "\n",
    "    rag_cache_path = os.path.join(LOCAL_CACHE_PATH, \"rag\")\n",
    "    donwload_dataset_nq_open_dpr(rag_cache_path)\n",
    "\n",
    "    PREBUILT_INDEX_NAME_BM25 = \"wikipedia-dpr-100w\"\n",
    "    LOCAL_INDEX_NAME_BM25 = os.path.join(rag_cache_path, \"lucene-index.wikipedia-dpr-100w.20210120.d1b9e6\")\n",
    "    # it might take a 10-20 minutes to download the index, recommand to use drive cache, if drive capacity is enough\n",
    "\n",
    "    try:\n",
    "        searcher_bm25 = LuceneSearcher(index_dir=LOCAL_INDEX_NAME_BM25)\n",
    "    except:\n",
    "        import shutil\n",
    "        searcher_bm25 = LuceneSearcher.from_prebuilt_index(prebuilt_index_name=PREBUILT_INDEX_NAME_BM25)\n",
    "        index_dir = searcher_bm25.index_dir\n",
    "        shutil.move(index_dir, LOCAL_INDEX_NAME_BM25)\n",
    "        searcher_bm25 = LuceneSearcher(index_dir=LOCAL_INDEX_NAME_BM25)\n",
    "\n",
    "\n",
    "    model_name_or_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    llm_model = transformers.AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                                                  torch_dtype=torch.bfloat16,\n",
    "                                                                  device_map={\"\": 0},\n",
    "                                                                  low_cpu_mem_usage=True,)\n",
    "    llm_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    llm_batch_size = 4\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "    llm_tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "    eval_dataset = RAGDataset(\n",
    "        tokenizer=llm_tokenizer,\n",
    "        is_train=False,\n",
    "        dataset_path=os.path.join(rag_cache_path,\"data\",\"nq_open_dpr\",\"nq_dev\"),\n",
    "    )\n",
    "    eval_collator = RAGCollator(\n",
    "        tokenizer=llm_tokenizer,\n",
    "        is_train=False,\n",
    "    )\n",
    "\n",
    "    model_rag = ModelRAG()\n",
    "    model_rag.set_model(llm_model)\n",
    "    model_rag.set_tokenizer(llm_tokenizer)\n",
    "    model_rag.set_retriever(searcher_bm25)\n",
    "    model_rag.make_augmented_inputs_for_generate = MethodType(cot_make_augmented_inputs, model_rag)\n",
    "\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=llm_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=eval_collator,\n",
    "    )\n",
    "    class rag_config:\n",
    "        device = \"cuda\"\n",
    "        model_dtype = \"cast_bf16\"\n",
    "\n",
    "    metrics = eval_for_rag(model_rag, eval_loader, rag_config, tokenizer)\n",
    "    prediction = metrics.pop(\"prediction\")\n",
    "    answers = metrics.pop(\"answers\")\n",
    "    uid = metrics.pop(\"uid\")\n",
    "    questions = metrics.pop(\"question\")\n",
    "    with open(os.path.join(RESULTS_PATH,\"llm_prompt_rag_score.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    with open(os.path.join(RESULTS_PATH,\"llm_prompt_rag_output.txt\"), \"w\") as f:\n",
    "        for u,q,p,a in zip(uid, questions, prediction, answers):\n",
    "            f.write(f\"{u}\\t{q}\\t{a}\\t{p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/06/2025 11:57:36 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "loading configuration file /mountpoint/lkj/NLP/output/summary/best_model/config.json\n",
      "Model config TransformerConfig {\n",
      "  \"architectures\": [\n",
      "    \"TransformerForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"ffn_dropout\": 0.05,\n",
      "  \"head_dim\": 32,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"custom_transformer\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 20000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"vocab_size\": 50258\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading weights file /mountpoint/lkj/NLP/output/summary/best_model/model.safetensors\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "All model checkpoint weights were used when initializing TransformerForCausalLM.\n",
      "\n",
      "All the weights of TransformerForCausalLM were initialized from the model checkpoint at /mountpoint/lkj/NLP/output/summary/best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TransformerForCausalLM for predictions without further training.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50266. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "06/06/2025 11:57:37 - INFO - __main__ - Initializing LORA model...\n",
      "trainable params: 425,984 || all params: 67,104,256 || trainable%: 0.6348\n",
      "Tokenizing and reformatting instruction data: 100%|â–ˆ| 43685/43685 [00:50<00:00, \n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43685/43685 [00:01<00:00, 27473.33 examples/s]\n",
      "06/06/2025 11:59:04 - INFO - __main__ - ***** Running training *****\n",
      "06/06/2025 11:59:04 - INFO - __main__ -   Num examples = 43685\n",
      "06/06/2025 11:59:04 - INFO - __main__ -   Num Epochs = 3\n",
      "06/06/2025 11:59:04 - INFO - __main__ -   Instantaneous batch size per device = 4\n",
      "06/06/2025 11:59:04 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "06/06/2025 11:59:04 - INFO - __main__ -   Gradient Accumulation steps = 2\n",
      "06/06/2025 11:59:04 - INFO - __main__ -   Total optimization steps = 16383\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16383/16383 [20:17<00:00, 12.78it/s]tokenizer config file saved in /mountpoint/lkj/NLP/output/sum_lora_finetuned/tokenizer_config.json\n",
      "Special tokens file saved in /mountpoint/lkj/NLP/output/sum_lora_finetuned/special_tokens_map.json\n",
      "loading configuration file /mountpoint/lkj/NLP/output/summary/best_model/config.json\n",
      "Model config TransformerConfig {\n",
      "  \"architectures\": [\n",
      "    \"TransformerForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"ffn_dropout\": 0.05,\n",
      "  \"head_dim\": 32,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"custom_transformer\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 20000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"vocab_size\": 50258\n",
      "}\n",
      "\n",
      "/home/guest2/anaconda3/envs/kangjun/lib/python3.12/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16383/16383 [20:18<00:00, 13.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Please change parameters for each environment.\n",
    "if DO_SUMMARY_LORA_FINETUNE:\n",
    "    !python finetune.py   --train_file train.jsonl   --model_name_or_path /mountpoint/lkj/NLP/output/summary/best_model   --tokenizer_name /mountpoint/lkj/NLP/output/summary/best_model   --output_dir /mountpoint/lkj/NLP/output/sum_lora_finetuned  --use_lora   --use_special_tokens   --num_train_epochs 3   --per_device_train_batch_size 4   --gradient_accumulation_steps 2   --learning_rate 5e-5   --max_seq_length 512   --overwrite_cache   --low_cpu_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/06/2025 11:26:13 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "loading configuration file /mountpoint/lkj/NLP/output/rag/best_model/config.json\n",
      "Model config TransformerConfig {\n",
      "  \"architectures\": [\n",
      "    \"TransformerForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"ffn_dropout\": 0.05,\n",
      "  \"head_dim\": 32,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"custom_transformer\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 20000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"vocab_size\": 50258\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading weights file /mountpoint/lkj/NLP/output/rag/best_model/model.safetensors\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "All model checkpoint weights were used when initializing TransformerForCausalLM.\n",
      "\n",
      "All the weights of TransformerForCausalLM were initialized from the model checkpoint at /mountpoint/lkj/NLP/output/rag/best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TransformerForCausalLM for predictions without further training.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50266. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "06/06/2025 11:26:15 - INFO - __main__ - Initializing LORA model...\n",
      "trainable params: 425,984 || all params: 67,104,256 || trainable%: 0.6348\n",
      "Tokenizing and reformatting instruction data: 100%|â–ˆ| 43685/43685 [00:48<00:00, \n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43685/43685 [00:01<00:00, 28108.39 examples/s]\n",
      "06/06/2025 11:27:40 - INFO - __main__ - Sample 5094 of the training set: {'input_ids': tensor([21017, 46486,    25,   198,  7841,   352,    13, 30396,   198,   818,\n",
      "          428,  4876,    11,   345,   821,  1813,   257,  5166,   286, 13439,\n",
      "           11,  6827,   352,   290,  6827,   362,    11,   326,  6159,  4236,\n",
      "          351,  4249, 18372,  1123,   584,    13,  3406,  1693,   318,   284,\n",
      "         8343,  6827,   362,   523,   326,   262,  5166,  4084, 18372,  1123,\n",
      "          584,    13,  2980,   515, 13439,  1276,   307,  1790,    11,   351,\n",
      "         1342,   621,  1315,  2456,    13,   968,  1321,   460,   307,  5495,\n",
      "           13, 24390,  1262, 43947,   284, 27531,   262,  2426,   286,   262,\n",
      "         6827,    13,   198,  7841,   362,    13, 17934,   198, 31837,   594,\n",
      "          352,    25,  5966,  2497,   465,  1545,  4186,  2406,   503,   286,\n",
      "          262, 16918,  3650,   351,   257,  6131,   286,  8234,    13, 11352,\n",
      "          594,   362,    25,  4186,   550,   587,  9735,   329,  8234,   284,\n",
      "         1577,  5966,    13,   198, 33706,    25,  4186,   550,  1239,   587,\n",
      "          287,   262,  3650,    13,   198,  3109, 11578,   341,    25,  4186,\n",
      "         1276,   423,  3750,   656,   262,  3650,   284,  1282,   503,   286,\n",
      "          340,    13,   198,  7841,   513,    13, 32900,   198, 31837,   594,\n",
      "          352,    25,  4930,  1466,   389,  5055,   287,   257,  3650,   351,\n",
      "          257,  5156,   287,   257,  4171,   336, 10646,    13, 11352,   594,\n",
      "          362,    25,   262,  1466,   389, 46711, 45512,   198, 33706,    25,\n",
      "          198,   198, 21017, 18261,    25,   198,    58,  2949,  4990,   380,\n",
      "        18206,    60,   464,  1466,   389,  2641,  3693, 18274,   879,    25,\n",
      "           20,    60, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,   198,    58,  2949,  4990,   380,\n",
      "        18206,    60,   464,  1466,   389,  2641,  3693, 18274,   879,    25,\n",
      "           20,    60, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n",
      "06/06/2025 11:27:40 - INFO - __main__ - Sample 27517 of the training set: {'input_ids': tensor([21017, 46486,    25,   198, 42758,   262, 23227,    82,   656,   734,\n",
      "        11570,  2628,    13,   198,   198,  9914,    11, 21516,    11, 14697,\n",
      "           11, 30799,   198,   198, 21017, 18261,    25,   198,    58,  2949,\n",
      "         4990,   380, 18206,    60, 10267,    82,    25,  1879,    11, 21516,\n",
      "           11, 30799,   220,   198, 35364, 26632,    25, 14697,    58, 18274,\n",
      "          879,    25,    20,    60, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,   198,    58,  2949,\n",
      "         4990,   380, 18206,    60, 10267,    82,    25,  1879,    11, 21516,\n",
      "           11, 30799,   220,   198, 35364, 26632,    25, 14697,    58, 18274,\n",
      "          879,    25,    20,    60, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])}.\n",
      "06/06/2025 11:27:40 - INFO - __main__ - Sample 620 of the training set: {'input_ids': tensor([21017, 46486,    25,   198,  2061,   714,  2245,   345,  1216,   261,\n",
      "        11090,   278,    30,   220,   317,    13,  2089,  2438,   220,   347,\n",
      "           13,  1487,   220,   327,    13,  1365,  3725,   220,   360,    13,\n",
      "        24902,   220,   412,    13,  4673,   517,   546,   220,   383,  1266,\n",
      "         3280,   318,   198,    33,   628,   198,     7, 24361,     8,   198,\n",
      "        41502, 15297,   318,   262,   923,    11,   788,   340,  2753, 16389,\n",
      "          284,   466,   644,    30,   220,   317,    13,  4911,  3511,   220,\n",
      "          347,    13,  2251,  1242,   220,   327,    13, 34249,   220,   360,\n",
      "           13,  3551,   220,   412,    13,  4911,   220,   383,  1266,  3280,\n",
      "          318,   198,     7, 33706,     8,   198,    33,   628,   198,    48,\n",
      "          947,    25,  1002,   345,   821,  7722,   644,  1767,   636,   857,\n",
      "          749,   286,   262,   670,    30,   220,   317,    13,  5405,   220,\n",
      "          347,    13, 45590,   220,   327,    13, 13589,   220,   360,    13,\n",
      "         5422,   220,   412,    13,   779,  5405,   220,   383,  1266,  3280,\n",
      "          318,   198,  2025,    82,    25,   360,   628,   198,     7,    48,\n",
      "          737,   198,  2061,   561,   661,   466,   611,   484,   765,   284,\n",
      "          423,  1257,    30,   220,   317,    13,  4589,  6741,   220,   347,\n",
      "           13,   636,  1664,   220,   327,    13, 14720,   220,   360,    13,\n",
      "          467, 14899,   220,   412,    13,  1975,   287,  5770,   220,   383,\n",
      "         1266,  3280,   318,   198,     7,    32,   737,   198,    35,   628,\n",
      "          198,    48,   947,    25,    39, 36846,   389,   925,   286,  6147,\n",
      "          290,   644,   584,  2378,    30,   220,   317,    13,  2705,  1108,\n",
      "          220,   347,    13,  7309,   220,   327,    13,  7815,   220,   360,\n",
      "           13, 13510,   220,   412,    13,  2891,  3524,   220,   383,  1266,\n",
      "         3280,   318,   198,   198,  2025,    82,    25,    35,   628,   198,\n",
      "         2061,   561,   779,   281,  1848,  1492,    30,   220,   317,    13,\n",
      "         6915, 33451,   220,   347,    13, 27517,   220,   327,    13,  3331,\n",
      "          220,   360,    13,  1394,  5504,   220,   412,    13,  1394,  2610,\n",
      "          286,  7237,   220,   383,  1266,  3280,   318,   198,   198, 21017,\n",
      "        18261,    25,   198,    58,  2949,  4990,   380, 18206,    60,    34,\n",
      "           58, 18274,   879,    25,    20,    60, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   198,    58,  2949,  4990,   380, 18206,    60,    34,\n",
      "           58, 18274,   879,    25,    20,    60, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n",
      "06/06/2025 11:27:41 - INFO - __main__ - ***** Running training *****\n",
      "06/06/2025 11:27:41 - INFO - __main__ -   Num examples = 43685\n",
      "06/06/2025 11:27:41 - INFO - __main__ -   Num Epochs = 3\n",
      "06/06/2025 11:27:41 - INFO - __main__ -   Instantaneous batch size per device = 4\n",
      "06/06/2025 11:27:41 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "06/06/2025 11:27:41 - INFO - __main__ -   Gradient Accumulation steps = 2\n",
      "06/06/2025 11:27:41 - INFO - __main__ -   Total optimization steps = 16383\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16383/16383 [20:14<00:00, 14.42it/s]tokenizer config file saved in /mountpoint/lkj/NLP/output/rag_lora_finetuned/tokenizer_config.json\n",
      "Special tokens file saved in /mountpoint/lkj/NLP/output/rag_lora_finetuned/special_tokens_map.json\n",
      "loading configuration file /mountpoint/lkj/NLP/output/rag/best_model/config.json\n",
      "Model config TransformerConfig {\n",
      "  \"architectures\": [\n",
      "    \"TransformerForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"ffn_dropout\": 0.05,\n",
      "  \"head_dim\": 32,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"custom_transformer\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 20000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"vocab_size\": 50258\n",
      "}\n",
      "\n",
      "/home/guest2/anaconda3/envs/kangjun/lib/python3.12/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16383/16383 [20:14<00:00, 13.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Please change parameters for each environment.\n",
    "if DO_RAG_LORA_FINETUNE:\n",
    "    !python finetune.py   --train_file train.jsonl   --model_name_or_path /mountpoint/lkj/NLP/output/rag/best_model   --tokenizer_name /mountpoint/lkj/NLP/output/rag/best_model   --output_dir /mountpoint/lkj/NLP/output/rag_lora_finetuned  --use_lora   --use_special_tokens   --num_train_epochs 3   --per_device_train_batch_size 4   --gradient_accumulation_steps 2   --learning_rate 5e-5   --max_seq_length 512   --overwrite_cache   --low_cpu_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-downloaded dataset from cache.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075fdb2b269d4c13987f16f6eb736dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/719 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## summary lora finetuned_model evaluation\n",
    "\n",
    "if DO_SUMMARY_LORA_FINETUNE:\n",
    "    from dataset.summary import prepare_summary_dataset, collate_fn_for_summary\n",
    "\n",
    "    summary_config = TrainingConfig(\n",
    "        device=\"cuda\",\n",
    "        batch_size=16,\n",
    "        eval_batch_size=16,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=3,\n",
    "        max_steps=None,\n",
    "        optimizer_type=\"adamw\",\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=1000,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        metric_for_best_model=\"loss\",\n",
    "        eval_interval=2500,\n",
    "        logging_interval=10,\n",
    "        save_total_limit=3,\n",
    "        logging_path=os.path.join(LOG_PATH, \"summary\"),\n",
    "        output_path=os.path.join(OUTPUT_PATH, \"summary\"),\n",
    "    )\n",
    "\n",
    "    summary_train, summary_eval = prepare_summary_dataset(tokenizer)\n",
    "    collate_fn = functools.partial(collate_fn_for_summary,\n",
    "                                    tokenizer=tokenizer)\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    from peft import PeftModel, PeftConfig\n",
    "\n",
    "    OUTPUT_PATH_2 = \"/mountpoint/lkj/NLP/output/sum_lora_finetuned\"\n",
    "\n",
    "    peft_config = PeftConfig.from_pretrained(OUTPUT_PATH_2)\n",
    "\n",
    "    base_model = TransformerForCausalLM.from_pretrained(\n",
    "        peft_config.base_model_name_or_path,  \n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(OUTPUT_PATH_2)\n",
    "\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # base modelì— adapter merge\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        OUTPUT_PATH_2,\n",
    "        adapter_name=\"default\", \n",
    "    )\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        summary_eval,\n",
    "        batch_size=summary_config.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    metrics = eval_for_summary(model, eval_loader, summary_config, tokenizer, max_new_tokens=128)\n",
    "    prediction = metrics.pop(\"prediction\")\n",
    "    answers = metrics.pop(\"answers\")\n",
    "    with open(os.path.join(RESULTS_PATH,\"summary_score_lora.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    with open(os.path.join(RESULTS_PATH,\"summary_output_lora.txt\"), \"w\") as f:\n",
    "        for p,a in zip(prediction, answers):\n",
    "            a = a.replace(\"\\n\", \" \").strip()\n",
    "            p = p.replace(\"\\n\", \" \").strip()\n",
    "            f.write(f\"{a}\\t{p}\\n\")\n",
    "\n",
    "    del summary_train, summary_eval, model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "TransformerForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e322dec1704bc08ea3364fca1de8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# rag lora finetuned model evaluation\n",
    "if DO_RAG_LORA_FINETUNE:\n",
    "    from model_rag import ModelRAG\n",
    "    from transformers import AutoTokenizer\n",
    "    from peft import PeftModel, PeftConfig\n",
    "\n",
    "    OUTPUT_PATH_2 = \"/mountpoint/lkj/NLP/output/pre_finetuned_rag\"\n",
    "\n",
    "    rag_config = TrainingConfig(\n",
    "            device=\"cuda\",\n",
    "            batch_size=16,\n",
    "            eval_batch_size=16,\n",
    "            gradient_accumulation_steps=1,\n",
    "            num_train_epochs=3,\n",
    "            max_steps=None,\n",
    "            optimizer_type=\"adamw\",\n",
    "            learning_rate=1e-4,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=300,\n",
    "            max_grad_norm=1.0,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            metric_greater_is_better=True,\n",
    "            eval_interval=2000,\n",
    "            logging_interval=10,\n",
    "            save_total_limit=3,\n",
    "            logging_path=os.path.join(LOG_PATH, \"rag\"),\n",
    "            output_path=os.path.join(OUTPUT_PATH, \"rag\"),\n",
    "        )\n",
    "\n",
    "    peft_config = PeftConfig.from_pretrained(OUTPUT_PATH_2)\n",
    "\n",
    "    base_model = TransformerForCausalLM.from_pretrained(\n",
    "        peft_config.base_model_name_or_path,  \n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(OUTPUT_PATH_2)\n",
    "\n",
    "    eval_dataset = RAGDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        is_train=False,\n",
    "        dataset_path=os.path.join(rag_cache_path,\"data\",\"nq_open_dpr\",\"nq_dev\"),\n",
    "    )\n",
    "    eval_collator = RAGCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        is_train=False,\n",
    "    )\n",
    "\n",
    "    model_rag = ModelRAG()\n",
    "    model_rag.set_model(model)\n",
    "    model_rag.set_tokenizer(tokenizer)\n",
    "    model_rag.set_retriever(searcher_bm25)\n",
    "\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=rag_config.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=eval_collator,\n",
    "    )\n",
    "    class rag_config:\n",
    "        device = \"cuda\"\n",
    "        model_dtype = \"cast_bf16\"\n",
    "\n",
    "    metrics = eval_for_rag(model_rag, eval_loader, rag_config, tokenizer)\n",
    "    prediction = metrics.pop(\"prediction\")\n",
    "    answers = metrics.pop(\"answers\")\n",
    "    uid = metrics.pop(\"uid\")\n",
    "    questions = metrics.pop(\"question\")\n",
    "    with open(os.path.join(RESULTS_PATH,\"pre_finetuned_rag_score.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    with open(os.path.join(RESULTS_PATH,\"pre_finetuned_rag_output.txt\"), \"w\") as f:\n",
    "        for u,q,p,a in zip(uid, questions, prediction, answers):\n",
    "            f.write(f\"{u}\\t{q}\\t{a}\\t{p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfxr7XCYhuU_"
   },
   "outputs": [],
   "source": [
    "if DO_SUBMISSION:\n",
    "    os.makedirs(\"submission\", exist_ok=True)\n",
    "    os.makedirs(\"submission/code\", exist_ok=True)\n",
    "\n",
    "    submission_code_list = [\n",
    "        \"main.ipynb\",\n",
    "        \"model.py\",\n",
    "        \"model_rag.py\",\n",
    "        \"model_lora.py\",\n",
    "        \"dataset/summary.py\",\n",
    "        \"dataset/rag.py\",\n",
    "        \"dataset/classification.py\",\n",
    "        \"dataset/pretrain.py\",\n",
    "        \"utils/etc.py\",\n",
    "        \"utils/logger.py\",\n",
    "        \"utils/metrics.py\",\n",
    "        \"finetune.py\",  \n",
    "        \"train.jsonl\", \n",
    "        \"README\"\n",
    "    ]\n",
    "\n",
    "    submission_directory_list = [\n",
    "        \"logs\",\n",
    "        RESULTS_PATH,\n",
    "        os.path.join(OUTPUT_PATH, \"pretraining\", \"best_model\"),\n",
    "        os.path.join(OUTPUT_PATH, \"summary\", \"best_model\"),\n",
    "        os.path.join(OUTPUT_PATH, \"classification\", \"best_model\"),\n",
    "        os.path.join(OUTPUT_PATH, \"rag\", \"best_model\"),\n",
    "        os.path.join(OUTPUT_PATH, \"rag_lora_finetuned\", \"best_model\"),\n",
    "        os.path.join(OUTPUT_PATH, \"sum_lora_finetuned\", \"best_model\"),\n",
    "        os.path.join(OUTPUT_PATH, \"pre_lora_finetuned\", \"best_model\"),\n",
    "        os.path.join(OUTPUT_PATH, \"pre_finetuned_rag\", \"best_model\"),\n",
    "    ]\n",
    "\n",
    "    for file in submission_code_list:\n",
    "        dst = os.path.join(\"submission\", \"code\", file)\n",
    "        dst_dir = os.path.dirname(dst)\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        if not os.path.exists(file):\n",
    "            with open(dst, \"w\") as f:\n",
    "                f.write(f\"{file} not exist\")\n",
    "        else:\n",
    "            shutil.copyfile(file, dst)\n",
    "\n",
    "    for directory in submission_directory_list:\n",
    "        if not os.path.exists(directory):\n",
    "            continue\n",
    "        base_name = os.path.basename(directory)\n",
    "        if 'best_model' in directory:\n",
    "            dirname = os.path.dirname(directory)\n",
    "            dst = os.path.join(\"submission\", dirname, base_name)\n",
    "        else:\n",
    "            dst = os.path.join(\"submission\", base_name)\n",
    "            \n",
    "        try:\n",
    "            if os.path.exists(dst) and os.path.samefile(directory, dst):\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            shutil.copytree(directory, dst, dirs_exist_ok=True)\n",
    "        except shutil.Error as e:\n",
    "            print(f\"Skip copying {directory} due to: {e}\")\n",
    "\n",
    "    shutil.make_archive(\"defaultproject_code\", 'zip', \"submission/code\")\n",
    "    shutil.rmtree(\"submission/code\", ignore_errors=True)\n",
    "\n",
    "    shutil.make_archive(\"defaultproject_supplementaries\", 'zip', \"submission\")\n",
    "    shutil.rmtree(\"submission\", ignore_errors=True)\n",
    "\n",
    "    shutil.move(\"defaultproject_code.zip\", os.path.join(PROJ_PATH, \"defaultproject_code.zip\"))\n",
    "    shutil.move(\"defaultproject_supplementaries.zip\", os.path.join(PROJ_PATH, \"defaultproject_supplementaries.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kangjun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "005aa25bb61f48ac8efb19f4512cebf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adb42c480d7f4773a1234c6061f4828f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ab44e6727c654c75bc507ba55f208a25",
      "value": "Downloadingâ€‡builderâ€‡script:â€‡100%"
     }
    },
    "010a83eb49dd44ea8a950f15e0f71e60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_daa478204caf4a74b85c0912a9c7457e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b2011c1e226f4f4ea82b46b1e55d0487",
      "value": "â€‡366/366â€‡[00:43&lt;00:00,â€‡â€‡1.75s/it]"
     }
    },
    "0410021dd1074dcb97f4cebac142019e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a0abe34d0724bcc99a9736e6cc3fef5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6a36e0397a0a4bd5a658670c30705a0c",
      "value": "â€‡365/366â€‡[00:16&lt;00:00,â€‡21.74it/s]"
     }
    },
    "04474e83860e4c01a10892961242b643": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b8cc575827741c9bb5af3bd81f245a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "0bea25eea440436cb217d897cdda46f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fbb12b2e38d0446186b3f23a37dd7580",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7da0e361ae50414badbdd5598e3c4425",
      "value": "Evaluating:â€‡100%"
     }
    },
    "0e36a466d1f84e3484a159a139769062": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b952e7a9b964b469ebfc3bd3a0ec6ad",
      "max": 93844,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d0fbec47818942d1a89ea4e84fe8cd65",
      "value": 21002
     }
    },
    "0fc0a19ce36a4f568a28e37beb61398e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10ae8e76f7c24459b08b5d2d2b635688": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "14f0cf5723f94ce2a54127fc8cc0f914": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f915f3c4f5df4a3680ed1aa2d770597d",
       "IPY_MODEL_b064703010da4e9ab933b2e5b34d297a",
       "IPY_MODEL_010a83eb49dd44ea8a950f15e0f71e60"
      ],
      "layout": "IPY_MODEL_3b94817d5e5d406e990070c3cc42c74b"
     }
    },
    "15794cc3490d43c0b513e05a25ec1c17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75ea11b89d5c489e87bba1d661ac1114",
      "max": 366,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7f34b3ae4dae4ed6a1543190eff71a5c",
      "value": 366
     }
    },
    "1820744ef80b4fe4b4c25a3bbcf2adc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2ad544a8ceca4c0ba4442beea11422f1",
       "IPY_MODEL_c99fd48965164a0086a824bde121b038",
       "IPY_MODEL_8e05c846028b449ca9f9deef6adacd07"
      ],
      "layout": "IPY_MODEL_5a9bab42b84d46f6aa229f13b31bccb7"
     }
    },
    "1a0abe34d0724bcc99a9736e6cc3fef5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1aefc69eb99a47328a4adec7cdb8b8c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c66d7a601bff4ae788800f69ea437e24",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_de44fd480b4f41bca98564310c506249",
      "value": "â€‡365/366â€‡[00:16&lt;00:00,â€‡21.70it/s]"
     }
    },
    "1b3ea0b85a274c1187603322a63b5cc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f72d7727c6d14b64916873e603b456be",
      "max": 366,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_10ae8e76f7c24459b08b5d2d2b635688",
      "value": 366
     }
    },
    "1ce6926c6f5e44f98ac15fbe927611be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fb18ced426147489838b9f9538dcae1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_005aa25bb61f48ac8efb19f4512cebf3",
       "IPY_MODEL_28d3585998804f6aa98d06db6e6d0e36",
       "IPY_MODEL_7ad0b2f49a1248e0a7819d0f3083e12a"
      ],
      "layout": "IPY_MODEL_2af23fa0a09941459d0995e913c0a9aa"
     }
    },
    "228762a0892b4175aa98d3351802e491": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa12c0684d214019a8b7cc3b86d64daa",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_40ffd473104e4613ae67974e779a8455",
      "value": "Evaluating:â€‡100%"
     }
    },
    "24bf3e46bdbd47e09197f2ee15c55e9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "253e1cb6466749a99a1af7fcf049afb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cfa1b9f20c4f4a01bc53b370ea422919",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_04474e83860e4c01a10892961242b643",
      "value": "â€‡21002/93844â€‡[27:23&lt;1:23:11,â€‡14.59it/s,â€‡epoch=0,â€‡loss=3.2088::3.1580,â€‡lr=2.69e-04]"
     }
    },
    "28260fa3f6fb42eea5ad7857b10ac396": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28d3585998804f6aa98d06db6e6d0e36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6100350a45274402a7c1dd7b9ce32336",
      "max": 6270,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bfc8e0b5310749c4a0be9a31b3e39218",
      "value": 6270
     }
    },
    "29840492a5644c5385a444dae097bfae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ad544a8ceca4c0ba4442beea11422f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac4569c968764541b5fe52401ff89710",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0fc0a19ce36a4f568a28e37beb61398e",
      "value": "Evaluating:â€‡100%"
     }
    },
    "2af23fa0a09941459d0995e913c0a9aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fb8d1ee9b474cfeafbd5890806b12ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "329914c1bc9045bd97852ecc9a74476e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "32fdfac1882b42a3bea99f8d5d48ea33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b69db0da8c7a42c9942eebd140724305",
       "IPY_MODEL_9bf31c36ea034ad689c5ff2d91cc7e7b",
       "IPY_MODEL_1aefc69eb99a47328a4adec7cdb8b8c4"
      ],
      "layout": "IPY_MODEL_329914c1bc9045bd97852ecc9a74476e"
     }
    },
    "395b126e4368481c9b09e90664019a07": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b6db537ab1540bc8e6a652d9873f1fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_45780dbe87f04aef8513c0762908b193",
       "IPY_MODEL_80bfbe62f733467f9c886cdd9745c1af",
       "IPY_MODEL_c0e619bbbcbc44049003cbbfba2949da"
      ],
      "layout": "IPY_MODEL_0b8cc575827741c9bb5af3bd81f245a5"
     }
    },
    "3b94817d5e5d406e990070c3cc42c74b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "3f9252fdc8984cf79abf54cc5d3f9ccf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "404321f63ebf497180d164b45d72965a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40d6667a9d244f8080d167635d906c16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40ffd473104e4613ae67974e779a8455": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4417e1f8aed74346b6b4d1a2167ce05b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "45780dbe87f04aef8513c0762908b193": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97ac312adfff4c358bdcc04217b089b9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_484698dce69c45218234bf91a12a8bde",
      "value": "Evaluating:â€‡100%"
     }
    },
    "47cfbe08701b4a8f8a9ebaf8cf2ccfed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_228762a0892b4175aa98d3351802e491",
       "IPY_MODEL_15794cc3490d43c0b513e05a25ec1c17",
       "IPY_MODEL_a51b76dc477d49d8a1b3d23b0d062ead"
      ],
      "layout": "IPY_MODEL_9ce17c3bef44415bba0b30c6a3b5dcb7"
     }
    },
    "484698dce69c45218234bf91a12a8bde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4de459958b1043dfacb0df5085a86474": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "536df2098f9b4077a82c77262e223b64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a9bab42b84d46f6aa229f13b31bccb7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "5adcded64bb04bc4a1f612558a3b06bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b22f7f1bdec468a893d1252fb74329b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0bea25eea440436cb217d897cdda46f6",
       "IPY_MODEL_1b3ea0b85a274c1187603322a63b5cc8",
       "IPY_MODEL_adcb2ec2de8c4576ac413b1029a36c16"
      ],
      "layout": "IPY_MODEL_c81876b78b174ded8e7ac565c3dcd32c"
     }
    },
    "5c8b1436a3bb41d08281ae2712eec063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b5e0b7b4efbb44b5aced06fb1882ca62",
       "IPY_MODEL_0e36a466d1f84e3484a159a139769062",
       "IPY_MODEL_253e1cb6466749a99a1af7fcf049afb3"
      ],
      "layout": "IPY_MODEL_3f9252fdc8984cf79abf54cc5d3f9ccf"
     }
    },
    "5d151df99fda4ae9963effdb64ba2f5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6100350a45274402a7c1dd7b9ce32336": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "612b0e3d5c2949ed811cfd7b4dea6a78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64f11ec2b1a24845b6da1bc691767ccc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a36e0397a0a4bd5a658670c30705a0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75ea11b89d5c489e87bba1d661ac1114": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ad0b2f49a1248e0a7819d0f3083e12a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc9bbc5ef68f45fdb439cdd062d54778",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b6d61efd5f7f4970bb4176dd9a72b1fe",
      "value": "â€‡6.27k/6.27kâ€‡[00:00&lt;00:00,â€‡631kB/s]"
     }
    },
    "7c35abbb646649288173a81a8e6b03df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29840492a5644c5385a444dae097bfae",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fbb9f0d02df44621825f9efafd59bcc9",
      "value": "Evaluating:â€‡100%"
     }
    },
    "7d86bbcd499a4faeabfffc3fe0bc1d47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7da0e361ae50414badbdd5598e3c4425": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f34b3ae4dae4ed6a1543190eff71a5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "80bfbe62f733467f9c886cdd9745c1af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_858ab8abb08749f9a9de3d2bc2392c71",
      "max": 366,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b10f63b7976045fcb70cc6349333c037",
      "value": 366
     }
    },
    "858ab8abb08749f9a9de3d2bc2392c71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bb5dce01a1d4c2ba0f44e31c6b7e1c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "8c54a104f9684547adf74275223d1c63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e05c846028b449ca9f9deef6adacd07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ce6926c6f5e44f98ac15fbe927611be",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2fb8d1ee9b474cfeafbd5890806b12ba",
      "value": "â€‡365/366â€‡[00:16&lt;00:00,â€‡21.77it/s]"
     }
    },
    "94c122b8f58948fbb715d7be1d568aad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cd71ffd73bf34584ae7003d110b61f0c",
       "IPY_MODEL_c21233b21b0d4c0293e16ef8f26e567b",
       "IPY_MODEL_0410021dd1074dcb97f4cebac142019e"
      ],
      "layout": "IPY_MODEL_8bb5dce01a1d4c2ba0f44e31c6b7e1c4"
     }
    },
    "959955c6a2314333ad1a8950a5bbd07a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97ac312adfff4c358bdcc04217b089b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b952e7a9b964b469ebfc3bd3a0ec6ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bf31c36ea034ad689c5ff2d91cc7e7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c54a104f9684547adf74275223d1c63",
      "max": 366,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_24bf3e46bdbd47e09197f2ee15c55e9f",
      "value": 366
     }
    },
    "9ce17c3bef44415bba0b30c6a3b5dcb7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "a33d26daec5c4b99be3f6ef0dbc973db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7c35abbb646649288173a81a8e6b03df",
       "IPY_MODEL_e135c8db8d67429eb936ce3d144d87df",
       "IPY_MODEL_ce8f3896f9354add8e9ac0c72e0aef40"
      ],
      "layout": "IPY_MODEL_4417e1f8aed74346b6b4d1a2167ce05b"
     }
    },
    "a5147c81b7354b52984147b157fe560d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a51b76dc477d49d8a1b3d23b0d062ead": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d151df99fda4ae9963effdb64ba2f5b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c2fb365b4348435493f782bf957c6f76",
      "value": "â€‡365/366â€‡[00:16&lt;00:00,â€‡21.81it/s]"
     }
    },
    "ab44e6727c654c75bc507ba55f208a25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac4569c968764541b5fe52401ff89710": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adb42c480d7f4773a1234c6061f4828f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adcb2ec2de8c4576ac413b1029a36c16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64f11ec2b1a24845b6da1bc691767ccc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e6d0e517ff5a408ebd3701de19bdb043",
      "value": "â€‡365/366â€‡[00:16&lt;00:00,â€‡21.79it/s]"
     }
    },
    "b064703010da4e9ab933b2e5b34d297a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_395b126e4368481c9b09e90664019a07",
      "max": 366,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca900706746546c198f9894f1109835c",
      "value": 366
     }
    },
    "b10f63b7976045fcb70cc6349333c037": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b2011c1e226f4f4ea82b46b1e55d0487": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b37e0c6f2db841bca4847d8b73be7b54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5e0b7b4efbb44b5aced06fb1882ca62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_959955c6a2314333ad1a8950a5bbd07a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a5147c81b7354b52984147b157fe560d",
      "value": "Training:â€‡â€‡22%"
     }
    },
    "b69db0da8c7a42c9942eebd140724305": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28260fa3f6fb42eea5ad7857b10ac396",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7d86bbcd499a4faeabfffc3fe0bc1d47",
      "value": "Evaluating:â€‡100%"
     }
    },
    "b6d61efd5f7f4970bb4176dd9a72b1fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bfc8e0b5310749c4a0be9a31b3e39218": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c0e619bbbcbc44049003cbbfba2949da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_404321f63ebf497180d164b45d72965a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ff5c6170ca08429094c3d2a868acd7d0",
      "value": "â€‡365/366â€‡[00:16&lt;00:00,â€‡21.80it/s]"
     }
    },
    "c21233b21b0d4c0293e16ef8f26e567b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40d6667a9d244f8080d167635d906c16",
      "max": 366,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d4a6b6bf301c4bc583ce4d5af2ee8c07",
      "value": 366
     }
    },
    "c2fb365b4348435493f782bf957c6f76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c66d7a601bff4ae788800f69ea437e24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c81876b78b174ded8e7ac565c3dcd32c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "c99fd48965164a0086a824bde121b038": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b37e0c6f2db841bca4847d8b73be7b54",
      "max": 366,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fd4ea03c73e745cca8e4e5d9447266be",
      "value": 366
     }
    },
    "ca900706746546c198f9894f1109835c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cd71ffd73bf34584ae7003d110b61f0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5adcded64bb04bc4a1f612558a3b06bd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_db59ff6f27364aacb8e56a664d8584ea",
      "value": "Evaluating:â€‡100%"
     }
    },
    "ce8f3896f9354add8e9ac0c72e0aef40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_612b0e3d5c2949ed811cfd7b4dea6a78",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e325ffd30eba46919eb67fc1931199c9",
      "value": "â€‡365/366â€‡[00:16&lt;00:00,â€‡21.91it/s]"
     }
    },
    "cfa1b9f20c4f4a01bc53b370ea422919": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0fbec47818942d1a89ea4e84fe8cd65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d4a6b6bf301c4bc583ce4d5af2ee8c07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d97238196deb4400b10f007adff0ed05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daa478204caf4a74b85c0912a9c7457e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db59ff6f27364aacb8e56a664d8584ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc9bbc5ef68f45fdb439cdd062d54778": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de44fd480b4f41bca98564310c506249": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e135c8db8d67429eb936ce3d144d87df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef996e77e5c54c25a66fa5df89a7aff9",
      "max": 366,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4de459958b1043dfacb0df5085a86474",
      "value": 366
     }
    },
    "e325ffd30eba46919eb67fc1931199c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6d0e517ff5a408ebd3701de19bdb043": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ef996e77e5c54c25a66fa5df89a7aff9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f72d7727c6d14b64916873e603b456be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f915f3c4f5df4a3680ed1aa2d770597d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d97238196deb4400b10f007adff0ed05",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_536df2098f9b4077a82c77262e223b64",
      "value": "Evaluating:â€‡100%"
     }
    },
    "fa12c0684d214019a8b7cc3b86d64daa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbb12b2e38d0446186b3f23a37dd7580": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbb9f0d02df44621825f9efafd59bcc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd4ea03c73e745cca8e4e5d9447266be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ff5c6170ca08429094c3d2a868acd7d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
